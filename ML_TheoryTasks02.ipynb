{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment No. 2: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1. Bayesian methods (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset $D =(X,y) =\\{(x_i,y_i)\\}^m_{i=1}$, $x_i \\in \\mathbb{R}^d$, $y_i\\in\\mathbb{R}$ it is known,that \n",
    "$$y_i = w^T x_i + \\epsilon$$\n",
    "where $\\epsilon \\sim N(0,\\sigma^2)$, $w  \\sim N(0,\\alpha I)$ . Suppose that $X^T X =I$, where $I$ is the identity matrix. Derive MAP estimation for $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - The MAP of w can be derived as shown below;\n",
    "   \n",
    "    \n",
    "$$ w^{*} = {arg\\,min}_{w}\\left(  \\frac{1}{2\\sigma^{2}}  \\sum^{m}_{i=1} (f(x_{i},w)-y_{i})^2  +  \\frac{1}{2\\alpha} w^{T}W \\right)  $$  \n",
    "\n",
    "\n",
    "    - The solution to the above equation is given as;\n",
    "\n",
    "\n",
    "$$W = {arg\\,min}_{w}\\left( \\frac{1}{2\\sigma^{2}} (XW-Y)^2  +  \\frac{1}{2\\alpha}\\left\\lVert W \\right\\rVert\n",
    " ^{2}\\right)  $$\n",
    "\n",
    "\n",
    "    - We obtain the optimal solution of the equation above by finding the derivative w.r.t to w \n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} = \\left( \\frac{1}{2\\sigma^{2}}\\times 2X^{T} (XW-Y) +  \\frac{1}{2\\alpha} \\times 2W \\right)  $$\n",
    "\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w} = \\left( \\frac{1}{\\sigma^{2}}\\times  (X^{T}XW-X^{T}Y) +  \\frac{W}{\\alpha} \\right)  $$\n",
    "\n",
    "    - Given that XTX = I; Then the above expresssion becomes;\n",
    "    \n",
    "$$\\frac{\\partial}{\\partial w} = \\left( \\frac{1}{\\sigma^{2}}(W-X^{T}Y) +  \\frac{W}{\\alpha} \\right)  $$\n",
    "\n",
    "    - We obtain the optimal, equate the derivative to 0\n",
    "    \n",
    "$$\\frac{\\partial}{\\partial w} = \\left( \\frac{1}{\\sigma^{2}}(W-X^{T}Y) +  \\frac{W}{\\alpha} \\right) = 0  $$\n",
    "   \n",
    "$$\\left( \\frac{W}{\\sigma^{2}} + \\frac{W}{\\alpha} - \\frac{X^{T}Y}{\\sigma^{2}} \\right) = 0  $$\n",
    "\n",
    "$$  W \\left( \\frac{\\alpha + \\sigma^{2}}{\\sigma^{2} \\alpha} \\right) =\\frac{X^{T}Y}{\\sigma^{2}}  $$\n",
    "\n",
    "   \n",
    "    - Therefore the value of MAP for w is given as ;\n",
    "    \n",
    "    \n",
    "$$  w =   \\frac{\\alpha X^{T}Y}{\\alpha + \\sigma^{2}}  $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Gaussian Processes 1 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\sigma_n(\\mathbf{x}_*)$ be a predictive variance at point $\\mathbf{x}_*$ of a Gaussian Process $f_n$ with zero mean and covariance $k(\\cdot,\\cdot)$ that was built using first $n$ training points.\n",
    "Prove that for $\\forall \\mathbf{x}_*$ it holds\n",
    "$$\n",
    "    \\sigma_{n}(\\mathbf{x}_*) \\leq \\sigma_{n-1}(\\mathbf{x}_*).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - The Predictive variance at point x, with n-training points in use can be expressesd using probability theory as; \n",
    "    \n",
    "    \n",
    "$$ \\sigma_{n}(\\mathbf{x}_*) =  k(\\mathbf{x}_*,\\mathbf{x}_*) - K^{*T}_{n} K^{-1}_{n}K^{*}_{n} $$  \n",
    "\n",
    "    -with Kn being the co-variance matrix; \n",
    "    - using the definition above, the original problem can be re-written as; \n",
    "    \n",
    "\n",
    "$$k(\\mathbf{x}_*,\\mathbf{x}_*) - K^{*T}_{n} K^{-1}_{n}K^{*}_{n} <\n",
    "    k(\\mathbf{x}_*,\\mathbf{x}_*) - K^{*T}_{n-1} K^{-1}_{n-1}K^{*}_{n-1}\n",
    "$$  \n",
    "\n",
    "    - The expression above can be simply as shown below; \n",
    "    \n",
    "$$k(\\mathbf{x}_*,\\mathbf{x}_*) -  k(\\mathbf{x}_*,\\mathbf{x}_*) - K^{*T}_{n} K^{-1}_{n}K^{*}_{n} < - K^{*T}_{n-1} K^{-1}_{n-1}K^{*}_{n-1}\n",
    "$$  \n",
    "\n",
    "$$- K^{*T}_{n} K^{-1}_{n}K^{*}_{n} < - K^{*T}_{n-1} K^{-1}_{n-1}K^{*}_{n-1}\n",
    "$$ \n",
    "    \n",
    "$$K^{*T}_{n} K^{-1}_{n}K^{*}_{n} > K^{*T}_{n-1} K^{-1}_{n-1}K^{*}_{n-1}\n",
    "$$     \n",
    "    \n",
    "  \n",
    "    - The covarince matrix can be written Kn  can be expressed as;\n",
    "    \n",
    "$$ K_{n} = \\begin{bmatrix}\n",
    "    K_{n-1} & K_{n-1} \\\\\n",
    "    K^{T}_{n-1} & K(x_{n},x_{n})\n",
    "  \\end{bmatrix} \n",
    "$$  \n",
    "\n",
    " - $K^{T}_{n}=(K(x_{1},x_{n}),K(x_{2},x_{n}),K(x_{3},x_{n}),\\dots,K(x_{n-1},x_{n}))   $\n",
    " \n",
    "        - This can be solved using block matrix in inversion formula     \n",
    "    \n",
    "$$ \\begin{bmatrix} P & Q \\\\ R & S\\\\ \\end{bmatrix} =\\begin{bmatrix} (P^{-1}+P^{-1}QMCP^{-1}) & (-P^{-1}QM) \\\\ (-MRP^{-1}) & M \\end{bmatrix} $$\n",
    "\n",
    "        - Applying the inversion formula to the covariance matrix previously defined, then we have,\n",
    "        \n",
    "\n",
    "$$K^{-1}_{n} =\\begin{bmatrix} (K^{-1}_{n-1}+ K^{-1}_{n-1}K_{n-1}MK^{T}_{n-1}K^{-1}_{n-1})  &  (-K^{-1}_{n-1}K_{n-1}M \\\\ -MK^{T}_{n-1}K^{-1}_{n-1} & M \\end{bmatrix}$$\n",
    "\n",
    "- M in this case equals; $\\left( K(x_{n},x_{n}) - K^{T}_{n-1}K^{-1}_{n-1}K_{n-1} \\right)^{-1}$ which is  a positive number\n",
    "\n",
    "\n",
    "\n",
    " - by making $K^{*T}_{n} = (K^{*}_{n-1},b^{*})^{T})$ , the evaluation of  of $K^{*T}_{n}K^{-1}_{n}K^{*}_{n}$  is given as\n",
    "\n",
    "$$K^{*T}_{n}K^{-1}_{n}K^{*}_{n}(K^{*}_{n-1} \\; b^{*})\\begin{bmatrix} (K^{-1}_{n-1}+ K^{-1}_{n-1}K_{n-1}MK^{T}_{n-1}K^{-1}_{n-1})  &  (-K^{-1}_{n-1}K_{n-1}M) \\\\ -MK^{T}_{n-1}K^{-1}_{n-1} & M \\end{bmatrix}  \\begin{bmatrix} K^{*}_{n-1} \\\\ b   \\end{bmatrix}$$\n",
    "\n",
    "        - Expansion of the expression above from the LHS gives  the result below;\n",
    "\n",
    "$$\\begin{bmatrix} (K^{*T}_{n-1}K^{-1}_{n-1}  +   K^{*T}_{n-1}K^{-1}_{n-1}K_{n-1}MK^{T}_{n-1}K^{-1}_{n-1} - bMK^{T}_{n-1}K^{-1}_{n-1})  &  (-K^{*T}_{n-1}K^{-1}_{n-1}K_{n-1}M  + bM) \\end{bmatrix} \\begin{bmatrix} K^{*}_{n-1} \\\\ b   \\end{bmatrix} $$\n",
    "\n",
    "\n",
    "        -  and then by performing 2 by 2 vec-vec multiplication on the above;\n",
    "$$K^{*T}_{n-1}K^{-1}_{n-1}K^{*}_{n-1}  +  K^{*T}_{n-1}K^{-1}_{n-1}K_{n-1}MK^{T}_{n-1}K^{-1}_{n-1}K^{*}_{n-1} - bMK^{T}_{n-1}K^{-1}_{n-1}K^{*}_{n-1}-\n",
    "bK^{*T}_{n-1}K^{-1}_{n-1}K_{n-1}M  + b^{2}M)$$   \n",
    "\n",
    "         The above expression can be simplify as ;\n",
    "         \n",
    "$$K^{*T}_{n-1}K^{-1}_{n-1}K^{*}_{n-1}  + M(r^{2}- 2abr + b^2)  $$          \n",
    "\n",
    "\n",
    "- $where \\;r = K^{*T}_{n-1}K^{-1}_{n-1}K_{n-1}$ \n",
    "\n",
    "            - using reverse qudratic expansion; Expression then becomes;\n",
    "            \n",
    "$$K^{*T}_{n-1}K^{-1}_{n-1}K^{*}_{n} = K^{*T}_{n-1}K^{-1}_{n-1}K^{*}_{n-1} + M(r-b)^{2}$$     \n",
    "\n",
    "            - The RHS of the equation above is ;\n",
    "$$K^{*T}_{n-1}K^{-1}_{n-1}K^{*}_{n-1} + M(r-b)^{2}   \\geq   K^{*T}_{n-1}K^{-1}_{n-1}K^{*}_{n-1} $$   \n",
    "\n",
    "            - Conclusively, this tells that the preeictive variance cannot increase when training with more data, therefore, we have;\n",
    "            \n",
    "$$\\sigma_{n}(x_{*}) < \\sigma_{n-1}(x_{*}) $$            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Gaussian Processes 2 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider you have gaussian distribution on $R$ with zero mean and differentiable by arguments covariation funtion $k(x, \\tilde{x})$.Get an expression for the correlation between the implementation of a Gaussian process  $y(x) âˆ¼ GP (0, k(x, x ^{\\prime}))$ and its derivative $\\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - This problem can be solve using the properties of linear operators; (since differenciation operator is linear operator)\n",
    "\n",
    "    - Let the expression for the covariance between y(xm) and its derivative be expressed as;   \n",
    "    \n",
    "$$COV\\left(y(x_{m}),\\frac{\\partial y(x)}{x}|x =x_{n}\\right)$$\n",
    "\n",
    "    - It should be noted that the derivative of a gaussian process is also a gaussian process\n",
    "    - by using the pro\n",
    "    \n",
    "    \n",
    "    \n",
    "$$ COV\\left(y(x_{m}),\\frac{\\partial y(x)}{x}|x =x_{n}\\right) = \\mathbb{E} \\left[y(x_{m}) \\left( \\frac{\\partial y(x)}{\\partial x}|x= x_{n}   \\right)  \\right]$$    \n",
    "\n",
    "\n",
    "    - The R.H.S expression then becomes;\n",
    "\n",
    "$$ \\mathbb{E} \\left[y(x_{m}) \\left( \\frac{\\partial y(x)}{\\partial x}|x = x_{n}\\right)\\right] = \\mathbb{E} \\left[ \\frac{\\partial y(x)y(x_{m})}{\\partial x}|x= x_{n} \\right]$$    \n",
    "\n",
    "    - and then we have this next permissible expression as;\n",
    "\n",
    "$$ \\mathbb{E} \\left[ \\frac{\\partial y(x)y(x_{m})}{\\partial x}|x= x_{n} \\right] =    \\frac{\\partial \\mathbb{E} \\left[ y(x)y(x_{m})\\right]}{\\partial x}|x= x_{n}  $$ \n",
    "\n",
    "\n",
    "$$\\frac{\\partial \\mathbb{E} \\left[ y(x)y(x_{m})\\right]}{\\partial x}|x= x_{n} =  \\frac{\\partial K(x_{m},x)}{\\partial x}|x = x_{n}$$ \n",
    "\n",
    "    - withThe expression above canbe used to express our covariance as; \n",
    "$ with\\; x = x_{m}\\; and \\; \\tilde x =x_{n} $    \n",
    "    \n",
    "$$ COV\\left(y(x),\\frac{\\partial y(\\tilde x)}{\\tilde x}\\right) = \\frac{\\partial K(x,\\tilde x)}{\\partial \\tilde x}$$   \n",
    "\n",
    "\n",
    "    - To obtain the variance for function observation\n",
    "    \n",
    "$$\\sigma^{2}y(x) = \\mathbb{E} [y(x)^{2}] = k(x,x)$$    \n",
    "    \n",
    "     - To obtain the variance for derivative observation\n",
    "    \n",
    "$$\\sigma^{2}y'( \\tilde x) = \\mathbb{E} \\left[   \\left( \\frac{\\partial y(\\tilde x)}{\\partial \\tilde x}   \\right)^{2}  \\right] = \\frac{\\partial^{2} k(\\tilde x,\\tilde x)}{\\partial^{2} \\tilde x}$$\n",
    "\n",
    "    - The expression for the correlation can be finally written as;\n",
    "    \n",
    "$$ COV\\left(y(x),\\frac{\\partial y(\\tilde x)}{\\tilde x}\\right) =  \\frac{1}{\\sqrt{\\frac{\\partial^{2} k(\\tilde x,\\tilde x)}{\\partial^{2} \\tilde x}k(x,x)} }    \\frac{\\partial^{2} k(\\tilde x,\\tilde x)}{\\partial^{2} \\tilde x}$$                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4. Kernel theory (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $K(x, x'):\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}$ be a PDS kernel,\n",
    "and $\\phi\\colon \\mathcal{X} \\to \\mathcal{H}$ its <b>unknown </b> feature mapping. For $x,x'\\in\\mathcal{X}$ derive the formula for the **distance** between $\n",
    "\\phi(x)$ and $\\phi(x')$ in $\\mathcal{H}$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Given a transformed feature space; its distance can be expressed as;\n",
    "\n",
    "$$\n",
    "\\| \\phi(x) - \\phi(x') \\|^2 = (\\phi(x) - \\phi(x'))^T(\\phi(x) - \\phi(x')) \n",
    "$$\n",
    "\n",
    "    - This can be futher expanded to give;\n",
    "    \n",
    "$$\n",
    "(\\phi(x) - \\phi(x'))^T(\\phi(x) - \\phi(x')) = \\phi^T(x) \\phi(x) - 2 \\phi^T(x) \\phi(x') + \\phi^T(x') \\phi(x') \n",
    "$$\n",
    "\n",
    "    - The above gives a final expression in terms of K is given as;  \n",
    "\n",
    "$$\n",
    "\\langle \\phi(x), \\phi(x) \\rangle - 2 \\langle \\phi(x), \\phi(x') \\rangle + \\langle \\phi(x'), \\phi(x') \\rangle = K(x, x) - 2 K(x, x') + K(x', x') \\dots (1)\n",
    "$$\n",
    "\n",
    "    - with K(x,x) being a positive definite kernel, then the Kernel-matrix is symmetric positive semi-definite\n",
    "        \n",
    "        \n",
    "$$\n",
    "K = \\begin{bmatrix} K(x,x) & K(x,x') \\\\ K(x',x) & K(x',x') \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "    - This SPSD property implies that det(K) >= 0, by getting he expression for \n",
    "    \n",
    "$$\n",
    "|K| = K(x,x) K(x',x') - K^2(x, x') \\geq 0\n",
    "$$ \n",
    " \n",
    "    - solving the inequality above; ... \n",
    "$$ \n",
    "K(x,x) K(x',x') \\geq  K^2(x, x') \n",
    "$$ \n",
    "\n",
    "    - and further expression results to; \n",
    "$$\n",
    " \\sqrt{K(x,x) K(x',x')}\\geq K(x, x') \\dots eqn(2)\n",
    "$$ \n",
    "    \n",
    "    - by substituing eqn(2) above into eqn(1), we have the expression below; \n",
    "$$            \n",
    "K(x,x) - 2K(x, x') + K(x', x')  \\geq K(x,x) - 2 \\sqrt{K(x,x) K(x',x')} + K(x',x')      \n",
    "$$            \n",
    " \n",
    "    - and by evaluating this immediate result.... \n",
    " \n",
    "$$ \n",
    "  \\left(\\sqrt{K(x,x)} - \\sqrt{K(x',x')}\\right)^2 \\geq 0\n",
    "$$\n",
    " \n",
    "    - therefore this validate the expression above; and we get...\n",
    "\n",
    "$$\n",
    "\\| \\phi(x) - \\phi(x') \\|^2 = K(x, x) - 2 K(x, x') + K(x', x')\n",
    "$$ \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5. Naive Gradient Boosting Regression (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given a regression dataset, consisting of 5 samples with 1-dimensional feature vector $X$ and scalar target vector $y \\in \\mathbb{R}$:\n",
    "\n",
    "|  x   |  y   | \n",
    "|:----:|:----:| \n",
    "|  10  |  1   | \n",
    "|  32  |  9   | \n",
    "|  46  |  13  | \n",
    "|  54  |  16  | \n",
    "|  63  |  23  | \n",
    "\n",
    "In this task you are asked to implement **3 steps of Gradient Boosting Regression** with decision tree stumps as the learners $h_0, h_1, h_2$. \n",
    "\n",
    "In order to complete this task:\n",
    "1. Refer to the slides on naive boosting for regression in **Lecture 8**.\n",
    "2. Assume that the initial model $f_0$ is the mean of the target vector $y$\n",
    "3. According to the algorithm on the boosting approach for regression from **1.**, compute the residuals\n",
    "4. Manually, find a suitable split among the $x_i$ for each decision tree weak model $h_t(X)$, which minimizes the loss function:\n",
    "\n",
    "$$L_{\\text{split_i}} = \\frac{\\text{Var}_{left\\_split}*N_{1} + \\text{Var}_{right\\_split}*N_{2}}{N_{1}+N_{2}}$$\n",
    "\n",
    "where  $\\text{Var}$ is the variance of the values contained in each leaf, $N_1$ is the number of target values $y$ in the left leaf, $N_{2}$ - in the right leaf\n",
    "\n",
    "5. Perform the Gradient Boosting step on the ensemble model $f_t$ with the resulting decision tree stump predictions (assume that the learning rate $lr=1.0$).\n",
    "\n",
    "**Note on Decision Tree Stumps:** A decision tree stump is a decision tree, which consists only of the root and its immediate leaves. In case of this task, at each iteration you are asked to consider 5 different variants of the decision tree stumps $h_t^i$ - one variant for each of the split candidates $x_i$. You should choose the variant that minimizes the loss written above. The two leaves of the tree are formed according to the rule:\n",
    "\n",
    "```python\n",
    "if x_i < split:\n",
    "    target_value -> left leaf\n",
    "elif x_i >= split:\n",
    "    target_value -> right leaf\n",
    "```\n",
    "**HINT:** Think about what should be `target_value` equal to in case of Gradient Boosting Regression.\n",
    "\n",
    "The prediction of decision tree stump $h_t(x_i)$ is the mean of the values of the according leaf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The task**:\n",
    "\n",
    "* Fill in the table - round the values of table up to the second digit after decimal point:\n",
    "\n",
    "\n",
    "|   x  |   y  |$f_0$|$$y - f_0$$|  $L$  |$h_0$ |$f_1$ |$$y-f_1$$|  $L$  |$h_1$|$f_2$  |$$y - f_2$$| $L$  |$h_2$|$F_3$ |\n",
    "|------|------|-----|-----------|-------|----- |----- |---------|-------|-----|-------|-----------|------|-----|------|\n",
    "|  10  |  1   |12.4 | -11.40    | 53.44 |-7.40 |  5.00| -4.00   | 16.93 |-1.42|  3.58 |  -2.58    | 8.91 |-2.58| 1.00 | \n",
    "|  32  |  9   |12.4 |  -3.40    | 20.95 |-7.40 |  5.00|  4.00   | 12.93 |-1.42|  3.58 |   5.42    | 7.24 | 0.65| 4.22 |\n",
    "|  46  |  13  |12.4 |   0.60    | 16.93 | 4.93 | 17.33| -4.33   | 16.93 |-1.42| 15.92 |  -2.92    | 7.57 | 0.65| 16.56|\n",
    "|  54  |  16  |12.4 |   3.60    | 19.83 | 4.93 | 17.33| -1.33   | 13.80 |-1.42| 15.92 |   0.08    | 8.90 | 0.65| 16.56|\n",
    "|  63  |  23  |12.4 |  10.60    | 25.35 | 4.93 | 17.33|  5.67   |  8.91 | 5.67| 23.00 |   0.00    | 8.91 | 0.65| 23.65|\n",
    "\n",
    "\n",
    "\n",
    "where $L$ is the loss, calculated by the formula for decision tree stumps above, for each of the 5 split variants of the decision tree stump at each iteration\n",
    "* Write down the splits (the feature values) you have found for each of the tree stumps\n",
    "\n",
    "* Insert the predictions of the full ensemble model and the split values, you have achieved after 3 iterations into the plotting cell below (**COPY AND PASTE** the last column from the table above and the splits list to the plotting cell below, instead of **#your solution**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean \n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [10.,32.,46.,54.,63.]\n",
    "y = [1.,9.,13.,16.,23.]\n",
    "\n",
    "df = np.array([X,y]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the stump loss...\n",
    "#R = residuals, \n",
    "#i_splits = index\n",
    "def L_stump(i,Rs):\n",
    "    n = Rs.shape[0]\n",
    "    r1 = Rs[:i]- Rs[:i].mean()\n",
    "    r2 = Rs[i:]- Rs[i:].mean()\n",
    "    loss = (((r1)**2).sum()+((r2)**2).sum())/n\n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the stump values...\n",
    "def v_stump(i,Rs):\n",
    "    n =  Rs.shape[0] \n",
    "    r_ = np.zeros(n)\n",
    "    r_[:i] = Rs[:i].mean()\n",
    "    r_[i:] = Rs[i:].mean()\n",
    "    return r_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples == the bosstrapped samples\n",
    "#split_i == split indexes\n",
    "Rs = np.zeros([df.shape[0], 4])\n",
    "Rs[:,0] = df[:,1] - df[:,1].mean()\n",
    "split_i = np.zeros(Rs.shape[1], dtype=int)\n",
    "samples = np.zeros(Rs.shape, dtype=float)\n",
    "samples[:,0] = df[:,1].mean()\n",
    "res_stump = np.zeros(Rs.shape, dtype=float)\n",
    "res_stump[:,0] = df[:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moninuola\\Documents\\New folder\\lib\\site-packages\\ipykernel_launcher.py:6: RuntimeWarning: Mean of empty slice.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53.44  0.    0.    0.    0.  ]\n",
      "0.0\n",
      "[53.44 20.95  0.    0.    0.  ]\n",
      "0.0\n",
      "[53.44       20.95       16.93333333  0.          0.        ]\n",
      "0.0\n",
      "[53.44       20.95       16.93333333 19.83333333  0.        ]\n",
      "0.0\n",
      "[53.44       20.95       16.93333333 19.83333333 25.35      ]\n",
      "16.933333333333334\n",
      "[16.93333333  0.          0.          0.          0.        ]\n",
      "0.0\n",
      "[16.93333333 12.93333333  0.          0.          0.        ]\n",
      "0.0\n",
      "[16.93333333 12.93333333 16.93333333  0.          0.        ]\n",
      "0.0\n",
      "[16.93333333 12.93333333 16.93333333 13.8037037   0.        ]\n",
      "0.0\n",
      "[16.93333333 12.93333333 16.93333333 13.8037037   8.90555556]\n",
      "8.905555555555555\n",
      "[8.90555556 0.         0.         0.         0.        ]\n",
      "0.0\n",
      "[8.90555556 7.23715278 0.         0.         0.        ]\n",
      "0.0\n",
      "[8.90555556 7.23715278 7.56759259 0.         0.        ]\n",
      "0.0\n",
      "[8.90555556 7.23715278 7.56759259 8.90439815 0.        ]\n",
      "0.0\n",
      "[8.90555556 7.23715278 7.56759259 8.90439815 8.90555556]\n",
      "7.237152777777777\n"
     ]
    }
   ],
   "source": [
    "for j in range(1,Rs.shape[1]):\n",
    "    split_values = np.zeros(Rs.shape[0])\n",
    "    for i in range(df.shape[0]):\n",
    "        split_values[i] = L_stump(i,Rs[:,j-1])\n",
    "        print(split_values)\n",
    "        split_i[j] = np.argmin(split_values)\n",
    "        print(split_values[split_i[j]])\n",
    "        res_stump[:,j] = v_stump(split_i[j],Rs[:,j-1])\n",
    "        samples[:,j] = (samples[:,j-1]+ res_stump[:,j])\n",
    "        Rs[:,j] = (Rs[:,j-1] - res_stump[:,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.4       ,  5.        ,  3.58333333,  1.        ],\n",
       "       [12.4       ,  5.        ,  3.58333333,  4.22916667],\n",
       "       [12.4       , 17.33333333, 15.91666667, 16.5625    ],\n",
       "       [12.4       , 17.33333333, 15.91666667, 16.5625    ],\n",
       "       [12.4       , 17.33333333, 23.        , 23.64583333]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-11.4       ,  -4.        ,  -2.58333333,   0.        ],\n",
       "       [ -3.4       ,   4.        ,   5.41666667,   4.77083333],\n",
       "       [  0.6       ,  -4.33333333,  -2.91666667,  -3.5625    ],\n",
       "       [  3.6       ,  -1.33333333,   0.08333333,  -0.5625    ],\n",
       "       [ 10.6       ,   5.66666667,   0.        ,  -0.64583333]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the residuals\n",
    "Rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.4       , -7.4       , -1.41666667, -2.58333333],\n",
       "       [12.4       , -7.4       , -1.41666667,  0.64583333],\n",
       "       [12.4       ,  4.93333333, -1.41666667,  0.64583333],\n",
       "       [12.4       ,  4.93333333, -1.41666667,  0.64583333],\n",
       "       [12.4       ,  4.93333333,  5.66666667,  0.64583333]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(0) :  [12.4 12.4 12.4 12.4 12.4]\n"
     ]
    }
   ],
   "source": [
    "#Getting f(0) from boostrap samples\n",
    "print(\"f(0) : \",samples[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss values(L):  [-11.4  -3.4   0.6   3.6  10.6]\n"
     ]
    }
   ],
   "source": [
    "#Loss_values:\n",
    "print(\"Loss values(L): \",Rs[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h(0) values:  [-7.4        -7.4         4.93333333  4.93333333  4.93333333]\n"
     ]
    }
   ],
   "source": [
    "#Loss_values:\n",
    "print(\"h(0) values: \",res_stump[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(1) values:  [ 5.          5.         17.33333333 17.33333333 17.33333333]\n"
     ]
    }
   ],
   "source": [
    "print(\"f(1) values: \",samples[:,1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y- f(1) values:  [-4.          4.         -4.33333333 -1.33333333  5.66666667]\n"
     ]
    }
   ],
   "source": [
    "#y-f(1)....\n",
    "print(\"y- f(1) values: \",Rs[:,1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h(1) values:  [-1.41666667 -1.41666667 -1.41666667 -1.41666667  5.66666667]\n"
     ]
    }
   ],
   "source": [
    "#Loss_values:\n",
    "print(\"h(1) values: \",res_stump[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2) values:  [ 3.58333333  3.58333333 15.91666667 15.91666667 23.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"f(2) values: \",samples[:,2])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y-f(2) [-2.58333333  5.41666667 -2.91666667  0.08333333  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"y-f(2)\",Rs[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h(2) values:  [-2.58333333  0.64583333  0.64583333  0.64583333  0.64583333]\n"
     ]
    }
   ],
   "source": [
    "print(\"h(2) values: \",res_stump[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F(3) values:  [ 1.          4.22916667 16.5625     16.5625     23.64583333]\n"
     ]
    }
   ],
   "source": [
    "print(\"F(3) values: \",samples[:,3])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(df[:,0],df[:,1])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(x,F,stumps):\n",
    "    x_range = np.arange(np.min(x), np.max(x)+1)\n",
    "    x_r = []\n",
    "    f_r = []\n",
    "    stmps = [0] + stumps + [np.inf]\n",
    "    for st in range(1,len(stmps)):\n",
    "        x_r.extend([list(group) for k, group in groupby(x_range, lambda x: x<stmps[st] and x>=stmps[st-1]) if k])\n",
    "        f_r.append([f_i for f_i,x_ii in zip(F,x) if x_ii<stmps[st] and x_ii>=stmps[st-1]])\n",
    "    F_to_plot = []\n",
    "    for ft in range(len(f_r)):\n",
    "        #assert len(f_r) == len(x_r)\n",
    "        if len(f_r[ft]) == 1:\n",
    "            F_to_plot.extend([f_r[ft][0]]*len(x_r[ft]))\n",
    "        elif len(f_r[ft]) > 1:\n",
    "            F_to_plot.extend([mean(f_r[ft])]*len(x_r[ft]))\n",
    "    return F_to_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOTTING CELL##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46., 63., 32.])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the split_values\n",
    "df[split_i[1:],0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F(3) values:  [ 1.          4.22916667 16.5625     16.5625     23.64583333]\n"
     ]
    }
   ],
   "source": [
    "print(\"F(3) values: \",samples[:,3])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = [10,32,46,54,63]\n",
    "y = [1, 9, 13, 16, 23]\n",
    "min_ = np.min(x)\n",
    "max_ = np.max(x)\n",
    "\n",
    "#note that the order of F(x_i) should be corresponding to the order of x_i in the table\n",
    "\n",
    "############ INSERT YOUR SOLUTION HERE###############\n",
    "####your solution#####\n",
    "F3 =  [1., 4.22916667, 16.5625 ,16.5625 , 23.64583333] \n",
    "####your solution####\n",
    "splits = [46., 63., 32.]\n",
    "x_range = np.arange(min_,max_+1)\n",
    "boosted_F_plot = plot_tree(x, F3, stumps = list(np.sort(splits)))\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,6))\n",
    "ax.scatter(x,y, label = 'original')\n",
    "ax.scatter(x, F3, label = 'predicted')\n",
    "ax.plot(x_range,boosted_F_plot,'--', linewidth=2, label = 'composite function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6. AdaBoost (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following cases,explain how AdaBoost, as given in **Lecture 7**, will treat a weak hypothesis $h_t$ with weighted error $N_t(h_t , w_t )$. Also, in each case, explain why this behavior takes place.\n",
    "1. $N_t = \\frac{1}{2}$\n",
    "2. $N_t > \\frac{1}{2}$\n",
    "3. $N_t = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - As discussed in lectures; the Final ensemble of classifers is given as;\n",
    "    \n",
    "$$f_{T}(x) = sign \\left[\\sum^{T}_{i=1} \\alpha_{t}h_{t}(x)     \\right] $$     \n",
    "\n",
    "\n",
    "    - where alpha= the weight of classifiers and its given as;\n",
    "$$\\alpha_{t} = \\frac{1}{2}\\log \\frac{1-N_{t}}{N_{t}}$$ \n",
    "\n",
    "\n",
    " -   $1^{st}$ :  with $N_{t} = \\frac{1}{2}$  \n",
    " \n",
    "$$\\alpha_{t} = \\frac{1}{2}\\log \\frac{1- \\frac{1}{2}}{\\frac{1}{2}} = \\frac{1}{2}\\log 1 = 0$$\n",
    "\n",
    "- with$\\alpha_{t} = 0$ (resulting from $N_{t} = \\frac{1}{2}$),\n",
    "\n",
    "        - This occurs mainly because the classifier has great rate of data misclassification.\n",
    "        - The AdaBoost does not take into account the contribution of that corresponding classifier in the final emsemble of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $2^{nd}$: with $N_{t} \\geq \\frac{1}{2}$, \n",
    "- by adding a constraint $k < \\frac{1}{2}$ as offset; then $N_{t} = \\frac{1}{2} + k$  \n",
    " \n",
    "$$\\alpha_{t} = \\frac{1}{2}\\log \\frac{1- \\frac{1}{2}- k}{\\frac{1}{2} + k} = \\frac{1}{2}\\log\\left( 1 - \\frac{2k}{ \\frac{1}{2}+k} \\right) \\leq 0$$\n",
    "\n",
    "- with $\\alpha_{t} =\\leq 0$ , the classifier is sure to give correct predictions with $\\tilde N_{t}= \\frac{1}{2}-k$ \n",
    "\n",
    "\n",
    "        - For this case, the classifier with this result will be substracted instead of being added \n",
    "        - it simply means taking into account the opposite contribution of the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $3^{rd}$: with $N_{t} = 0$, we solve the alpha eqn using the limit approach\n",
    "\n",
    " \n",
    "$$\\alpha_{t} = \\frac{1}{2}\\lim_{N_{t}\\rightarrow +0} \\log \\frac{1- N_{t}}{N_{t}} \n",
    "\\rightarrow +\\infty$$\n",
    "\n",
    "\n",
    "\n",
    "- $N_{t} = 0$ implies zero-weighted classifier error, (no mistake in the classification process) \n",
    "     \n",
    "        - The AdaBoost gives the highest possible weight to this classifier in the emsembling process\n",
    "         - This only occurs when we have a perfect classifier which is practically impossible unless there is an occurrence of  overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
