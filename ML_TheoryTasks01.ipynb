{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home Assignment No. 1: Part 2 (Theory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the homework you are to solve several simple theoretical problems related to machine learning algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Linear Regression (1 point)\n",
    "Let us consider the problem of linear regression for 2D data $(x_{1},y_{1}),\\dots,(x_{n},y_{n})\\in\\mathbb{R}^{2+ 1}$. Let us have $l_{\\infty}$ regularization penalty, i.e. the optimization problem is\n",
    "$$\n",
    "||Xw - y||_2^2 + \\lambda||w||_{\\infty} \\rightarrow \\min_{\\boldsymbol{w}}\n",
    "$$\n",
    "\n",
    "Show that this problem is equal to Lasso regression problem with feature matrix $Z = XA \\in \\mathbb{R}^{n \\times 2}$ for a certain $2 \\times 2$ matrix $A$ and the same target $y$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -  From the geometry of L1 and L_inf regularization; when we apply a rotation matrix A to X such that\n",
    "    \n",
    "$$  A= \\sqrt 2\n",
    "  \\begin{bmatrix}\n",
    "    cos(45) & -sin(45)  \\\\\n",
    "    sin(45) &  cos(45) \n",
    "  \\end{bmatrix}     $$   \n",
    "  \n",
    "  \n",
    "        - The matrix above gives the rotation of the L_inf norm by 45-deg, and scaling factor of sqrt(2);\n",
    "        \n",
    "  - putting $Z = AX$  into the initial eqn, then $||w||_{\\infty} \\rightarrow ||w||_{1}$ \n",
    "\n",
    "$$||AXw - y||_2^2 + \\lambda||w||_{1} \\rightarrow \\min_{\\boldsymbol{w^{'}}}$$\n",
    "    \n",
    "        - This optimization is now of the form;\n",
    "       \n",
    "$$||Zw - y||_2^2 + \\lambda||w||_{1} \\rightarrow \\min_{\\boldsymbol{w^{'}}}$$   \n",
    "\n",
    "        - which is the lasso regression problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Probit Regression (1 point)\n",
    "Let us consider the Probit regression model for a binary classification problem. It is a linear model analogous to the Logistic Regression. For a feature vector $x \\in \\mathbb{R}^d$ the probability for label $y$ being equal to 1 is given by\n",
    "$$P(y=1|x) = \\Phi(x^Tw + b).$$ \n",
    "Here $\\Phi(x)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt$ is the Cumulative Density Function for the **standard normal distribution**; values $w \\in \\mathbb{R}^d$ and $b \\in \\mathbb{R}$ are learnable parameters of the probit regression model.\n",
    "\n",
    "Write down the optimization problem for training probit  probit regression **without L2-regularization** and show that it does not have optimum in the case when the training set is **lineary separable**.\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - This problem is reduced to showing that we can increase the log-likelihood indefinitely \n",
    "      when the training set is separable;\n",
    "    \n",
    "    - the Log-likehood of w is expressed as;\n",
    "    \n",
    "$$P(y=1|x)= \\Phi(yx^Tw); for \\; y \\in {[-1;1]} $$    \n",
    "\n",
    "    - If y = -1, the expression becomes; \n",
    "    \n",
    "$$P(y=-1|x)= 1 - P(y=1|x)$$\n",
    "\n",
    "$$P(y=-1|x) = 1 - \\Phi(x^Tw) = \\Phi(-x^Tw) = \\Phi(yx^Tw) $$   \n",
    "\n",
    "\n",
    "    - From the given commulative Density function;\n",
    "    \n",
    "$$\\Phi(x)=\\int_{-\\infty}^{x}\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^{2}}{2}}dt$$\n",
    "\n",
    "    - As seen above; f(t) = is symmetric function,therefore we have\n",
    "    \n",
    "$$L_{w} =  \\sum^{n}_{i = 1} log(P(y_{i}|x_{i}) = \\sum^{n}_{i = 1} log(\\Phi(y_{i}x_{i}^Tw)) \\rightarrow max_{w} $$    \n",
    "\n",
    "\n",
    "    - with the hyperplane (w^) that separate classes; then\n",
    "    \n",
    "$$y_{i}x^T_{i}\\hat{w} > 0; \\;for\\; all\\; i $$    \n",
    "\n",
    "    - Now considering the separating hyperplane \n",
    "\n",
    "$Separating\\; Hyperplane;\\lambda \\hat(w)$  \n",
    "\n",
    "$$\\lim_{\\lambda \\rightarrow +\\infty} L(\\lambda \\hat(w))=\\lim_{\\lambda \\rightarrow +\\infty} \\sum^{n}_{i=1} log(\\Phi(\\lambda y_{i}x^{T}_{i}\\hat(w)) = 0 $$\n",
    "\n",
    "    \n",
    "    - This problem therefore does not have an optimum as we take the indefinite limit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Multiclass Bayesian Naive Classifier (1 point)\n",
    "Let us consider multiclass classification problem with classes $C_1, \\ldots, C_K$. Assume that all $d$ features $x_1, \\dots,x_d$ are binary, i.e. $x_{i}\\in\\{0,1\\}$ for $i=1,2\\dots,d$. Show that the decision rule of a Bayesian Naive Classifier can be represented as $\\arg \\max$ of linear functions of the input. \n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - With binary d-features in mind; the Bernoulli Naive Classifier can be expressed as;\n",
    "    \n",
    "$$p(x_i,i\\in i|y=k)=\\prod_{i\\in d}p_{ki}^{x_i}(1-p_{ki})^{(1-x_i)} $$\n",
    "\n",
    "    - The MAP [maximum a posteriori] decision rule can be written as:\n",
    "\n",
    "$$\\hat{y}=arg \\max_{y\\in\\{C_{1}...C_{K}\\}}p(y|\\boldsymbol{x})=p(y=k)\\cdot\\prod_{i\\in d}p_{ki}^{x_i}(1-p_{ki})^{(1-x_i)}$$\n",
    "\n",
    "    - Re-writing the form in log :\n",
    "\n",
    "$$\\hat{y}=arg \\max_{y\\in\\{C_{1}...C_{K}\\}}log(p(y|\\boldsymbol{x}))=log(p(y=k)) + \\sum_{i\\in d}x_ilog(p_{ki})+\\sum_{i\\in d}(1-x_i)log(1-p_{ki})$$\n",
    "\n",
    "    - and this becomes \n",
    "\n",
    "$$\\hat{y}=arg \\max_{y\\in\\{C_{1}...C_{K}\\}}log(p(y|\\boldsymbol{x}))=log(p(y=k)) + \\sum_{i\\in d}log(1-p_{ki}) + \\sum_{i\\in d}x_ilog(p_{ki}) - \\sum_{i\\in d}x_ilog(1-p_{ki})$$\n",
    "\n",
    "    - Further expression of the eqn above becomes;\n",
    "\n",
    "$$\\hat{y}=arg \\max_{y\\in\\{C_{1}...C_{K}\\}}log(p(y|\\boldsymbol{x}))=log(p(y=k)) + \\sum_{i\\in d}log(1-p_{ki}) + \\sum_{i\\in d}x_i(log(p_{ki}) - log(1-p_{ki}))$$\n",
    "\n",
    "\n",
    "    - Indices into the last part of the equation\n",
    "    \n",
    "$$\\hat{y}=arg \\max_{y\\in\\{C_{1}...C_{K}\\}}log(p(y|\\boldsymbol{x}))=log(p(y=k)) + \\sum_{i\\in d}log(1-p_{ki}) + \\sum_{i\\in d}x_ilog(\\frac{p_{ki}}{1-p_{ki}}) $$\n",
    "\n",
    "    - Expressing our result into linear expression with parameters;\n",
    "$b = (log(p(y=k)) + \\sum_{i\\in d}log(1-p_{ki}), i\\in d)$;  \n",
    "$w = (log(\\frac{p_{ki}}{1-p_{ki}}, i\\in d)$\n",
    "$\\boldsymbol{x_i} = (x_i, i\\in d)$\n",
    "    \n",
    "    \n",
    "    \n",
    "$$\\hat{y}=arg \\max_{y\\in\\{C_{1}...C_{K}\\}}[b + w\\boldsymbol{x_i^T}] $$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Nearest Neighbors (1 point)\n",
    "Consider the 1-nearest-neighbor classifier applied to multiclass classification problem. Let's denote the classifier fitted on data $X$ by $f_X(\\cdot)$.\n",
    "\n",
    "The formula for complete **leave-k-out cross-validation** on data sample $X^{n}$ is defined as\n",
    "\n",
    "$$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\bigg[\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg)\\bigg],$$\n",
    "\n",
    "\n",
    "where $\\mathcal{P}(X^{n})$ is the set of all subsets of $X^{n}$. For all $i=1,2\\dots,n$ denote the label of $m$-th closest neighbor of $x_{i}$ in $X^{n}\\setminus \\{x_{i}\\}$ by $y_{i}^{m}$. Show that \n",
    "$$L_{k}OCV=\\sum_{m=1}^{k}\\underbrace{\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq y_{i}^{m}]}_{K_{m}(X^{n})}\\frac{C_{n-1-m}^{n-k-1}}{C_{n-1}^{k-1}},$$\n",
    "where $K_{m}(X^{n})$ is called the compactness profile of $X^{n}$, i.e. the fraction of objects whose $m$-th nearest neighbor has different label. For convenience assume that all the pairwise distances between objects are different.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Given the formula for the complete leave-k-out cross-validation below;\n",
    "\n",
    "$$L_{k}OCV=\\frac{1}{C_{n}^{k}}\\bigg[\\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq f_{X^{n}\\setminus X}( x_{i})]\\bigg)\\bigg],$$\n",
    "\n",
    "    -Re-writing the above formular in terms the label of the m-th closest neigbhor y{m_i} explained in the question;\n",
    "\n",
    "$$ L_{k}OCV= \\frac{1}{C_{n}^{k}}\\bigg[ \\sum\\limits_{X\\subset \\mathcal{P}(X^{n})\\wedge |X|=k}\\frac{1}{k}\\bigg(\\sum_{i\\in X}[y_{i}\\neq y_i^{m_i}]\\bigg)\\bigg]$$\n",
    "\n",
    "\n",
    "    - To evaluate the number of subsets inthe data sample with \"m-1\" closet neighbors; [m-1] objects fall into the main subset. \n",
    "    \n",
    "$$  = [m-k-1]\\; of\\; (x_{i} \\not\\in  X')$$\n",
    "    \n",
    "who is not in $X'$ from n-m-1(we know that $m-th$ closest neighbours not in $X'$). \n",
    "    \n",
    "    - by taking [n-k-1] elements in the main subset, and [n-m-1] elements in another subset.\n",
    "    \n",
    "    -and with m-th neighbor being in another class and belongs to the training subset, then number of partitions is;\n",
    "    \n",
    "$$ N_{partitions}= C_{n-m-1}^{n-k-1}$$\n",
    "\n",
    "    - Moving forward, re-writing  the complete leave-k-out cross-validation as;\n",
    "\n",
    "$$L_{k}OCV=\\sum_{i=1}^{n}\\frac{1}{C_n^kk}\\sum_{m=1}^{k}C_{n-m-1}^{n-k-1}[y_i\\neq y_i^m]$$\n",
    "\n",
    "    - Further expression of transformation gives;\n",
    "\n",
    "$$L_{k}OCV = \\sum_{m=1}^{k}\\frac{C_{n-m-1}^{n-k-1}}{C_n^kK}\\sum_{i=1}^{n}[y_i\\neq y_i^m]$$\n",
    "\n",
    "    - Therefore the final expression gives \n",
    "\n",
    "$$ L_{k}OCV = \\sum_{m=1}^{k}\\frac{1}{n}\\sum_{i=1}^{n}[y_i\\neq y_i^m]\\frac{C_{n-k-1}^{n-1-m}}{C_{n-1}^{k-1}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Bootstrap (1 point)\n",
    "Let the subsample $\\hat{X}^{n}$ of size $n$ be generated from sample $X^{n}=\\{\\boldsymbol{x}_{1},\\dots\\boldsymbol{x}_{n}\\}$ by bootstrap procedure. Find the probability that object $x_{i}$ is not present in $\\hat{X}^{n}$ and compute the limit of this probability for $n\\rightarrow\\infty$.\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - The probability of object not present in subsample is given by;\n",
    "    \n",
    "\n",
    "$$ P\\left(x_i \\text{ not in } \\hat{X}^n\\right) =  \\left( \\frac{(n-1)^n}{n^n} \\right) $$\n",
    "\n",
    "    - The expression above by indices becomes; \n",
    "\n",
    "$$ P\\left(x_i \\text{not in} \\hat{X}^n\\right) = \\left( \\frac{n-1}{n} \\right)^n $$\n",
    "\n",
    "\n",
    "    - The evaluation of the \"lim to inf\"of the expression above;\n",
    "    \n",
    "\n",
    "$$ \\rightarrow \\lim_{n\\rightarrow \\infty} \\left( \\frac{n-1}{n} \\right)^n  =   \\lim_{n\\rightarrow \\infty} \\left(1 - \\frac{1}{n} \\right)^n  $$\n",
    "\n",
    "    - Further expression the lim function gives;\n",
    "\n",
    "$$ \\lim_{n\\rightarrow \\infty} \\left(1 - \\frac{1}{n} \\right)^n  = \\lim_{n\\rightarrow \\infty}e^{n\\ln\\left(1-\\frac{1}{n}\\right)} = e^{\\lim_{n\\rightarrow \\infty}\\frac{\\ln(1-n^{-1})}{n^{-1}}}$$\n",
    " \n",
    " \n",
    "    - Simplyfying the superscript  of exp above;\n",
    "$$\\lim_{n\\rightarrow \\infty}\\frac{\\ln(1-n^{-1})}{n^{-1}} = \\lim_{n\\rightarrow \\infty}\\frac{\\frac{1}{n^2}\\frac{1}{1-\\frac{1}{n}}}{-\\frac{1}{n^2}}= \\lim_{n\\rightarrow \\infty}\\frac{-n}{n-1}$$    \n",
    "    \n",
    "    - Recalling that -inf/+inf = -1;\n",
    "$$\\lim_{n\\rightarrow \\infty}\\frac{-n}{n-1} = -1 $$  \n",
    "\n",
    "    - Putting this back to the exp equation;\n",
    "$$  \\lim_{n\\rightarrow \\infty} \\left(1 - \\frac{1}{n} \\right)^n = e^{-1} \\approx 0.3678 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Decision Tree Leaves (1+1=2 points)\n",
    "\n",
    "Consider a leaf of a binary decision tree which consists of object-label pairs $(x_{1},y_{1}),\\dots,(x_{n},y_{n})$. The prediction $\\hat{y}$ of this leaf is defined to minimize the loss on the training sample.\n",
    "\n",
    "* For a classification tree for K classes ($y_{i}\\in\\{1,2,\\dots,K\\}$) and zero-one loss $L(y,\\hat{y})=[y\\neq \\hat{y}]$, find the optimal prediction in the leaf.\n",
    "\n",
    "* For a regression tree ($y_{i}\\in\\mathbb{R}$) and squared percentage error loss $L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$ find the optimal prediction in the leaf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    -1]  For a classification tree for K classes and zero-one loss, the optimal prediction in the leaf is given by:\n",
    "    \n",
    "$$Optimal\\; leaf\\;Prediction = Mode\\;of\\;the\\;labels (i.e\\;the\\;Most\\;frequent\\;label)$$    \n",
    "\n",
    "    - And this is given mathematically as;\n",
    "    \n",
    "$$\\hat{y}=\\underset{k=1,\\dots,K}{\\operatorname{argmax}}\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}=k]$$    \n",
    "\n",
    "\n",
    "    -2] For a regression tree and squared percentage error loss, the optimal prediction in the leaf is given by;\n",
    "    \n",
    "$$L(y,\\hat{y})=\\frac{(y-\\hat{y})^{2}}{y^2}$$\n",
    "\n",
    "\n",
    "    - Finding the optimal leaf prediction; leads to evaluting the optimization problem below;\n",
    "    \n",
    "$$\\frac{\\partial L_{y,\\hat{y}}}{\\partial \\hat{y}} = \\frac{\\partial \\sum_{i=1}^{n}\\frac{(y_{i}-\\hat{y})^2}{y^{2}_{i}}} {\\partial \\hat{y}} =   \\sum_{i=1}^{n} -2\\frac{y_{i}-\\hat{y}}{y^{2}_{i}} = 0 $$\n",
    "\n",
    "\n",
    "\n",
    "    - To find  y^ \n",
    "    \n",
    "   \n",
    "$$\\sum_{i=1}^{n} (\\frac{2\\hat{y}}{y^{2}_{i}} - \\frac{2}{y_{i}}) =   2\\hat{y} \\sum_{i=1}^{n} \\frac{1}{y^{2}_{i}} -  2\\sum_{i=1}^{n} \\frac{1}{y_{i}} =  0 $$    \n",
    "   \n",
    "\n",
    "    -Further expression gives;\n",
    "    \n",
    "$$\\hat{y} \\sum_{i=1}^{n} \\frac{1}{y^{2}_{i}} -  \\sum_{i=1}^{n} \\frac{1}{y_{i}} =  0 $$        \n",
    " \n",
    "- And this gives y^ as;\n",
    "\n",
    "\n",
    " \n",
    "$$\\hat{y} = \\frac{\\sum_{i=1}^{n}\\frac{1}{y_{i}}}{\\sum_{i=1}^{n}\\frac{1}{y^{2}_{i}}} $$    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Classification (1 point)\n",
    "Let objects $\\boldsymbol{x}_{1},\\dots,\\boldsymbol{x}_{n}$ have binary labels $y_{1}, y_{2},\\dots,y_{n}\\in\\{0,1\\}$. Let the classifier $f$ assign to objects probabilities of being from class $1$. Without loss of generality assume that $f(\\boldsymbol{x_{i}})<f(\\boldsymbol{x_{j}})$ for all $i<j$. Denote the number of objects of $X^{n}$ from class $1$ by $n_{1}=\\sum_{i=1}^{n}[y_{i}=1]$. Show that \n",
    "$$S_{\\text{ROC}}(f,X^{n})=\\frac{1}{n_{1}(n-n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$\n",
    "where $S_{\\text{ROC}}(f,X^{n})$ is the Area Under ROC of classifier $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "    \n",
    "    - Let the class catergory represented as;\n",
    "\n",
    "\n",
    "$$Class_1 = n_{1} =\\sum_{i=1}^{n}[y_{i}=1]$$\n",
    "$$Class_0 = n_{0} = \\sum_{i=1}^{n}[y_{i}=0] = n-n_{1}$$\n",
    "\n",
    "\n",
    "    - Let the binary classifier be represented by;\n",
    "    \n",
    "    \n",
    "$$f (y_{n}\\in\\{0,1\\}) = sign[f(x) - f_{0}] : f_{0} = prediction \\;threshold$$\n",
    "\n",
    "\n",
    "\n",
    "    - The dependence of True Positive Rate(TPR) and False Positive Rate with threshold f0 is ahown by the ROC\n",
    "\n",
    "$$  If f_{0} > \\max_i(f(x_i)): then\\; TPR = FPR = 0; \\left( i.e \\;All\\;Objects--> Class_{0} \\right) $$\n",
    "\n",
    "$$If f_{0} < \\max_i(f(x_i)):then\\; TPR = FPR = 1; \\left( i.e \\;All\\;Objects--> Class_{1}\\right) $$\n",
    "\n",
    "    - Object intersect f0 for case_1 above, then;\n",
    "    \n",
    "$$if \\; y_{i} = 1; \\; TPR \\uparrow  by \\frac{1}{n_{1}}$$\n",
    "\n",
    "$$and \\;if\\; y_{i} = 0; \\; FPR \\uparrow  by \\frac{1}{n_{0}}$$\n",
    "\n",
    "    - By definition, the AUC can be expressed as; ;\n",
    "    \n",
    "$$AUC = \\int_{\\infty}^{-\\infty} TPR(T)FPR(T)dT$$\n",
    "    \n",
    "    - Combining the aforementioned deatils, the AUC can be expressed as;\n",
    "    \n",
    "$$AUC =\\frac{1}{n_{1}(n_{0})}\\sum_{i<j}[y_{i}<y_{j}]$$    \n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "      - with n0 = n-n1, then the expression becomes; \n",
    "$$  S_{\\text{ROC}}(f,X^{n}) = AUC =\\frac{1}{n_{1}(n - n_{1})}\\sum_{i<j}[y_{i}<y_{j}]$$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8. Kernels (1+1=2 points)\n",
    "Kernel $K(x,y)$ corresponds to dot product of feature maps $\\varphi$ and therefore $K(x,y) = \\langle \\varphi(x), \\varphi(y) \\rangle$. Derive functions $\\varphi$ for the following kernels:\n",
    "* $K(x,y)=\\langle x, y \\rangle ^ d$;\n",
    "* $K(x,y)= \\left(c + \\langle x, y \\rangle \\right)^ d$  with $c\\geq 0$;\n",
    "\n",
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $K(x,y)=\\langle x, y \\rangle ^ d$;\n",
    "\n",
    "\n",
    "        - The above expression is same as;\n",
    "     \n",
    "$$K(x,y) = \\langle x, y \\rangle ^ d = (x^T, y)^ d = \\left(\\sum^{m}_{i=1} x_{i}y_{i}\\right)^d $$\n",
    "\n",
    "       - Using the multinomial expansion; we then obtain; \n",
    "\n",
    "\n",
    "$$ K(x,y) =\\left(\\sum^{m}_{i=1} x_{i}y_{i}\\right)^d = \\left( x_{1}y_{1} + x_{2}y_{2} ....+ x_{m} y_{m} \\right)^d $$\n",
    "    \n",
    "    \n",
    "$$K(x,y) = x^{d}_{1} y^{d}_{1} + dx^{d-1}_{1}y^{d-1}_{1}x_{2}y_{2}+ \\frac{d(d-1)}{2} x^{d-2}_{1}y^{d-2}_{1}x_{2}y_{2} x_{3}y_{3}+ .....\n",
    "$$\n",
    "    \n",
    "    \n",
    "$$\n",
    " K(x,y) =  \\sum_{k_{1} + k_{2} +...+ k_{m} = d} \\binom {d} {k_{1}+k_{2}+...+k_{m}} \\prod^{m}_{t =1}x^{k_{t}}_{t}y^{k_{t}}_{t} =\\langle \\varphi(x), \\varphi(y) \\rangle \n",
    "$$\n",
    "\n",
    "        - Then we have ;\n",
    "\n",
    "$$\\varphi(x) = \\left( \\sqrt{\\binom {d} {k_{1}+k_{2}+...+k_{m}}}  \\prod^{m}_{t =1}x^{k_{t}}_{t} \\right)$$\n",
    "\n",
    "\n",
    "    For {k_{1}+k_{2}+...+k_{m} = d \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* $K(x,y)= \\left(c + \\langle x, y \\rangle \\right)^ d$\n",
    "\n",
    "        - Expansion of the expression above gives ;\n",
    "     \n",
    "$$K(x,y) = (c + x_{1}y_{1} +  x_{2}y_{2} + ........+ x_{m}y_{m})^d $$\n",
    "\n",
    "       - Using the multinomial expansion; we then obtain; \n",
    "\n",
    "\n",
    "$$K(x,y)= c^d + x^{d}_{1} y^{d}_{1} + .....+ x^{d}_{m} y^{d}_{m} + dc^{d-1} x_{1} y_{1}+....+d(d-1)c^{d-2} x^{2}_{1} y^{2}_{1}+ .... $$\n",
    "       \n",
    "     \n",
    "$$\n",
    "K(x,y)= \\sum_{k_{0} + k_{1} + k_{2}...+ k_{m} = d} \\binom {n} {k_{0}+k_{1}+ k_{2}+....+k_{m}} c^{k_{0}} \\prod^{m}_{t =1}x^{k_{t}}_{t}y^{k_{t}}_{t} = \\langle \\varphi(x), \\varphi(y) \\rangle\n",
    "$$\n",
    "\n",
    "    - Then we have ;\n",
    "    \n",
    "    \n",
    "\n",
    "$$\n",
    "\\varphi(x) = \\left(\\sqrt{\\binom {n} {k_{0}+k_{1}+ k_{2}...+ k_{m}}c^{k_{0}}} \\prod^{m}_{t=1}x^{k_{t}}_{t}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9. Kernel Methods (1 point)\n",
    "Prove that Gaussian Kernel $K(x,x')=\\exp(-\\|x-x'\\|^{2})$ is positive definite. You do not need to prove that the linear kernel is positive definite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Using the \"Cauchy-Schwarz inequality\" shown below; \n",
    "    \n",
    "$$K(x,x')^2 \\le K(x,x)K(x',x')$$\n",
    "\n",
    "    - The inequality above is satisfied for PDS kernel for any x,x' \n",
    "\n",
    "    -To show that Gaussian Kernel satisfies this inequality,considering the Guassian function form; \n",
    "$$g(t) = exp(-t^2), \\; interval\\;[0,1]$$ \n",
    "\n",
    "    - Values of g(t) is within [0,1] with [g(t)_max = 1] , at [t=0]\n",
    "\n",
    "    - Now considering the case given above, with;\n",
    "\n",
    "$$t =\\|x-x'\\|^{2}\\; max\\;value\\; is\\; when\\; x \\;= \\;x'$$\n",
    "\n",
    "    - The condition above is satified by diagonal elements of Mat_K, and for  K(x,x'); \n",
    "$$K(x,x')=\\exp(-\\|x-x'\\|^{2}) $$\n",
    "$$Diag(K(x,x')) = 1\\;and \\;NonDiag\\;<\\;1$$\n",
    " \n",
    "    -This allow that for any x,x' the Cauchy-Schwarz inequality will be satisfied and therefore proves that the Gaussian Kernel is Positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10. Support Vector Machine (1 point)\n",
    "Show that for two-class SVM classifier the following upper bound on accuracy leave-one-out-cross-validation estimate holds true:\n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$\n",
    "where for all $i=1,\\dots,n$ $f_{i}(x_{i})$ is SVM fitted on the entire data without observation $(x_{i},y_{i})$ and $|SV|$ is the number of support vectors of SVM fit on the entire data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - From the geometry understanding of SVM hyperplane for 2- class SVM ;\n",
    "    \n",
    "    - CASE_1: If we remove a data point that is not a [Support Vector] for the leave-one-out-cross-validation; \n",
    "    \n",
    "                - The hyperplane remain unchanged, and there is no error in the classification.\n",
    "    \n",
    "    - CASE_2: However, if we choose a data point [that is a support Vector] for the leave-one-out-cross-validation;\n",
    "    \n",
    "                - The hyperplane changes and this support vector datapoint may or maynot be classified correctly  \n",
    "    \n",
    "     - And so, the error of classification can not  exceed the number of support vectors that we have\n",
    "     \n",
    "     - because we undertake this process n-times; our accuracy is bounded, we then have\n",
    "   \n",
    "$$L_{1}OCV=\\frac{1}{n}\\sum_{i=1}^{n}[y_{i}\\neq_{i}(x_{i})]\\leq \\frac{|SV|}{n},$$   \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
