{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ye_cuFLrN7J3"
   },
   "source": [
    "# Home Assignment No. 4: Part 1 (Practice)\n",
    "To solve this task, you will write some code to try random **recurrent neural networks** in action. This part of the assignment is a simple practice with pytorch.\n",
    "* You are **HIGHLY RECOMMENDED** to read relevant documentation, e.g. for [python](https://docs.python.org/3/), [numpy](https://docs.scipy.org/doc/numpy/reference/), [matlpotlib](https://matplotlib.org/) and [pytorch](http://pytorch.org). Also remember that seminars, lecture slides, [Google](http://google.com) and [StackOverflow](https://stackoverflow.com/) are your close friends during this course (and, probably, whole life?).\n",
    "\n",
    "* In some problems you are asked to provide short discussion of the results. In these cases you have to create **MARKDOWN** cell with your comments right after the corresponding code cell.\n",
    "\n",
    "* For every separate (sub)problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**. So make sure that you did everything required in the task\n",
    "\n",
    "* Your **SOLUTION** notebook **MUST BE REPRODUCIBLE**, i.e. if the reviewer decides to execute all, after all the computation he will obtain exactly the same solution (with all the corresponding plots) as in your uploaded notebook.\n",
    "\n",
    "* Your code must be clear to the reviewer. For this purpose, try to include neccessary comments inside the code. But remember: **GOOD CODE MUST BE SELF-EXPLANATORY** without any additional comments.\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "During the seminar, we worked on building an intution for RNNs. We saw that they are very powerful techniques and compared to ARMA methods, where some assumptions must hold to apply the model, RNNs can be applied on sequential data as blackboxes and lead to good results.\n",
    "\n",
    "Recurrent Neural Networks (RNN) can read inputs $x^{\\langle t \\rangle}$ one by one. Through the hidden layer activations, they can \"remember\" some information. The RNN can pass it to the next step, in the case of the unidirectional RNN, or to previous and future steps in the bidirectional RNN.\n",
    "\n",
    "Before continuing, we should define some notation:\n",
    "* $[l]$ represents an object associated with the $l^{th}$ layer. \n",
    "* Whereas $(i)$ denotes an object associated with the $i^{th}$ example. \n",
    "\n",
    "- Superscript $\\langle t \\rangle$ denotes an object at the $t^{th}$ time-step. \n",
    "    \n",
    "- **Sub**script $i$ denotes the $i^{th}$ entry of a vector.\n",
    "\n",
    "Example:  \n",
    "- $a^{(2)[3]<4>}_5$ denotes the activation of the 2nd training example (2), 3rd layer [3], 4th time step <4>, and 5th entry in the vector.\n",
    "\n",
    "We first define some functions that will be helpful for us in the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bym2fl5cN7KL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4s_g4EzN7Kz"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = torch.exp(x - torch.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E2tVjI9ON7K_"
   },
   "source": [
    "The RNN can be intuitively understood as following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-vC7sATSN7LN"
   },
   "source": [
    "# **<img src=\"RNN_1.gif\" style=\"width:500;height:300px;\">**\n",
    "<caption><center>Figure 1: The Recurrent Neural Network</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SH1F48PxN7LT"
   },
   "source": [
    "Let's discuss this implementation more in detail.\n",
    "## Inputs and outputs of a RNN\n",
    "We start with the two input vectors of the RNN, the input vector and the hidden state.\n",
    "### Input $x$\n",
    "The number of units is represented as $n_x$. It can be understood as the number of units in a single timestep of a single training example.\n",
    "\n",
    "A timestep in a single input example is represented as $x^{(i) \\langle t \\rangle }$. The timesteps are indexed as $t$. Normally, a single training example $x^{(i)}$ will consist of multiple time steps $T_x$.\n",
    "\n",
    "The training examples are grouped into so-called mini batches $x$ of size $m$. Thus, the shape of a mini batch is $(n_x,m,T_x)$. $x$ is thus a 3-dimension tensor and this will be our input into the RNN.\n",
    "\n",
    "As we will be working with mini-batches of training examples, we will need a 2D slide $x^{\\langle t \\rangle}$ of the same shape of our mini batch, $(n_x, m)$. In our implementation, this will be the variable `xt`.\n",
    "### Hidden State $a$\n",
    "The second input required in a RNN is the hidden state $a^{\\langle t \\rangle}$, an activation function. The hidden state is passed from one step to the next. It shares similarities with our input vector $x$. \n",
    "\n",
    "For example, it has a length $n_{a}$, and if we include a mini-batch of size $m$, the shape of the mini-batch will be $(n_{a},m)$. The hidden state also makes use of the same index $t$ to iterate and on each loop it uses a 2D slice $a^{\\langle t \\rangle}$ of shape $(n_{a}, m)$.\n",
    "\n",
    "In our implementation, we will have two variables for the hidden state. They will be called `a_prev` and `a_next`.\n",
    "Similarly, on each step, we produce an output vector.\n",
    "### Output $\\hat{y}$\n",
    "The ouput $\\hat{y}$ is also a 3D tensor. Its shape is $(n_{y}, m, T_{y})$, where $n_{y}$ represents the number of units in the vector representing the prediction. As it is the case with the input, $m$ reflects the number of examples in the mini batch and $T_{y}$ the number of steps in the prediction. In our implementation, we will be defining this variable as `y_pred`.\n",
    "\n",
    "If we go more in depth and look at a single time step $t$, we have a 2D slice $\\hat{y}^{\\langle t \\rangle}$ with shape $(n_{y}, m)$. Similarly, the variable in our code for this 2D slice will be called `yt_pred`.\n",
    "## The RNN cell\n",
    "In the animation above, we see that the RNN model consists basically of looping over a single cell over the index $t$. As we outlined above, we take the input consisting of two vectors, the current input $x^{\\langle t \\rangle}$, and the previous hidden state $a^{\\langle t - 1\\rangle}$. \n",
    "At the same time, we will be passing to the next RNN cell two outputs. They are the hidden state  $a^{\\langle t \\rangle}$ and the prediction $\\hat{y}^{\\langle t \\rangle}$.\n",
    "Let's start by implementing the RNN cell!\n",
    "## Task 1. Simple Cell (4 points)\n",
    "In this task, you will have 4 individual exercises:\n",
    "* **(1 pt.)** First, start by implementing the hidden state $a$ with the tanh activation function. It is defined as \n",
    "$$\n",
    "\\begin{align}\n",
    "a^{\\langle t \\rangle} = \\tanh(W_{aa} a^{\\langle t-1 \\rangle} + W_{ax} x^{\\langle t \\rangle} + b_a).\n",
    "\\end{align}\n",
    "$$\n",
    "* **(1 pt.)** Second, use the obtained result in the first task to compute the prediction \n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}^{\\langle t \\rangle} = softmax(W_{ya} a^{\\langle t \\rangle} + b_y).\n",
    "\\end{align}\n",
    "$$\n",
    "* **(1 pt.)** Third, proceceed to store the tuple $(a^{\\langle t \\rangle}, a^{\\langle t-1 \\rangle}, x^{\\langle t \\rangle}, parameters)$ in a `accumulator` variable.\n",
    "* **(1 pt.)** Fourth return `accumulator` and $a^{\\langle t \\rangle}$ , $\\hat{y}^{\\langle t \\rangle}$.\n",
    "The function to implement consists of the following arguments:\n",
    "* `xt` - This is the input data at timestep $t$, a pytorch tensor of shape $(n_x, m)$\n",
    "* `a_prev` - Hidden state at timestep \"t-1\", a pytorch tensor of shape $(n_a, m)$\n",
    "* `parameters` - This is a python dictionary. It consists of five keys:\n",
    "* - Wax - The weight matrix multiplying the input $x$ with shape $(n_a, n_x)$\n",
    "* - Waa - The meight matrix multiplying the hidden state $a$ with shape $(n_a, n_a)$\n",
    "* - Wya - The weight matrix for the hidden-state to the output with shape $(n_y, n_a)$\n",
    "* - ba -  Represents the bias with shape $(n_a, 1)$\n",
    "* - by -  Similarly, this is the bias for the hidden-state to the output. It has a shape $(n_y, 1)$\n",
    "    \n",
    "The function must return the following variables:\n",
    "* `a_next` - This is the next hidden state with shape $(n_a, m)$\n",
    "* `yt_pred` - It is the prediction at timestep $t$ with shape $(n_y, m)$\n",
    "* `accumulator` - A tuple of values required for the backward pass. The tuple must contain (`a_next`, `a_prev`, `xt`, `parameters`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4TpYkgFN7La"
   },
   "outputs": [],
   "source": [
    "def rnn_cell_forward(xt, a_prev, parameters):    \n",
    "\n",
    "    Wax = parameters[\"Wax\"]\n",
    "    Waa = parameters[\"Waa\"]\n",
    "    Wya = parameters[\"Wya\"]\n",
    "    ba = parameters[\"ba\"]\n",
    "    by = parameters[\"by\"]\n",
    "    \n",
    "    ### WRITE SOLUTION HERE ###\n",
    "\n",
    "    #1....hidden_state: a with \"tahn-activation function\"\n",
    "    a_next = torch.tanh(torch.matmul(Waa,a_prev) + torch.matmul(Wax,xt) + ba)\n",
    "    \n",
    "    #2...prediction: y(t)\n",
    "    yt_pred = softmax(torch.matmul(Wya, a_next) + by) \n",
    "\n",
    "    #3....store tuples in accumulator             \n",
    "    accumulator = (a_next,a_prev,xt,parameters) \n",
    "\n",
    "    #4....return\n",
    "    return a_next,yt_pred,accumulator\n",
    "    \n",
    "    ### WRITE SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r7qW4tye018I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-vXn-5juN7Lr"
   },
   "source": [
    "Let's put our implementation to test! For this, we will define a time series with seasonality using the functions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KAluLVTlN7Lw"
   },
   "source": [
    "Let's also define some variables to control our series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d1CP8-0gN7L6"
   },
   "source": [
    "## The RNN Forward Pass\n",
    "In the previous task, we computed one iteration of a RNN cells. However, the RNN consists of a repetition of RNN cells. For each step in index $t$, we have to use the RNN cell again. \n",
    "\n",
    "Let's not forget that on each step, the RNN cell takes two inputs, the current input data $x^{\\langle t \\rangle}$ and the hidden state from the previous cell, $a^{\\langle t-1 \\rangle}$. The output, therefore, has to be two vectors, the prediction ($y^{\\langle t \\rangle}$) and the hidden state ($a^{\\langle t \\rangle}$). If you remember our previous task, we defined a set of weights and biases named $(W_{aa}, b_{a}, W_{ax}, b_{x})$. They will be used on each time step as well.\n",
    "\n",
    "Let's implement a forward pass for our RNN!\n",
    "\n",
    "## Task 2. Go Forward (1+1+1+1+1=5 points)\n",
    "This task will consist of 5 simple exercises. They are the following:\n",
    "* **(1 pt.)** First, initializing a 3D array $a$ with zeros. It must have the shape $(n_{a}, m, T_{x})$. This array must store all the hidden states computed by our RNN\n",
    "* **(1 pt.)** Second, similarly, we will need a 3D array to store the predictions. The array $\\hat{y}$ must have the same shape as the array $a$, this means a shape $(n_{y}, m, T_{x})$\n",
    "* **(1 pt.)** Third, please create the 2D hidden state `a_next`. For this, you must set it equal to $a_{0}$, the initial hidden state.\n",
    "* **(1 pt.)** Fourth, you must iterate through each step $t$. In each loop, the RNN must do the following:\n",
    "* - Retrieve $x^{\\langle t \\rangle}$ with shape $(n_{x}, m)$. This is the 2D slide of $x$ at a time step $t$\n",
    "* - Call the function `rnn_cell_forward` to update:\n",
    "* -- the hidden state `a_next` with shape $(n_{a}, m)$\n",
    "* -- the prediction $\\hat{y}^{\\langle t \\rangle}$\n",
    "* -- and the accumulator\n",
    "* - At the $t^{th}$ position:\n",
    "* -- Store the 2D hidden state in the 3D tensor $a$ with shape $(n_{a}, m, T_{x})$\n",
    "* -- Store the 2D prediction `yt_pred` with shape $(n_{y}, m)$ in the 3D tensor $\\hat{y}_{pred}$ with shape $(n_{y}, m, T_x)$\n",
    "* - Append the accumulator to a list of accumulators\n",
    "* - Store the input data `yt_pred` and the list of accumulator into a tuple `acc`\n",
    "* **(1 pt.)** Fifth, return three variables, the tuple `acc`, the 3D tensor $a$ and $\\hat{y}$\n",
    "\n",
    "Let's remember that to implement the function rnn_forward, we have access to the following variables and parameters:\n",
    "* `x` - This is the input data for all time steps, a pytorch tensor of shape $(n_x, m, T_x)$\n",
    "* `a0` - The initial hidden state at timestep \"0\", a pytorch tensor of shape $(n_a, m)$\n",
    "* `parameters` - This is a python dictionary. It consists of five keys:\n",
    "* - Wax - The weight matrix multiplying the input $x$ with shape $(n_a, n_x)$\n",
    "* - Waa - The meight matrix multiplying the hidden state $a$ with shape $(n_a, n_a)$\n",
    "* - Wya - The weight matrix for the hidden-state to the output with shape $(n_y, n_a)$\n",
    "* - ba -  Represents the bias with shape $(n_a, 1)$\n",
    "* - by -  Similarly, this is the bias for the hidden-state to the output. It has a shape $(n_y, 1)$\n",
    "    \n",
    "The function must return the following variables:\n",
    "* `a` - This is the next hidden state for all time steps with shape $(n_a, m,T_x)$\n",
    "* `y_pred` - It is the prediction for all time steps with shape $(n_y, m, T_x)$\n",
    "* `acc` - A tuple of values required for the backward pass. The tuple must contain (list of accumulators, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTaXGCUUN7L-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2aHPjn8nfcEN"
   },
   "outputs": [],
   "source": [
    "# a = torch.zeros([3,10,4]) \n",
    "# a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXOv79YlSKTA"
   },
   "outputs": [],
   "source": [
    "def rnn_forward(x, a0, parameters):\n",
    "    list_accumulators = []\n",
    "    n_x, m, T_x = x.shape\n",
    "    n_y, n_a = parameters[\"Wya\"].shape\n",
    "    \n",
    "    ### WRITE SOLUTION HERE ###\n",
    "    a = torch.zeros([n_a,m,T_x])       #1...Initiate 3D matrix-\"a\"\n",
    "    y_pred = torch.zeros([n_y,m,T_x])  #2...Initiate 3D for prediction-\"y\"\n",
    "    a_next = a0                        #3...create 2D hidden state \"a_next\"\n",
    "\n",
    "    for t in range(T_x):              #4...iterate through each step t \n",
    "                                      #.....Call the function \"rnn_cell_forward\"\n",
    "      a_next, yt_pred, accumulator = rnn_cell_forward(x[:,:,t],a_next,parameters) \n",
    "      a[:,:,t] = a_next                    #.....the hidden state \"a_next\"\n",
    "      y_pred[:,:,t] = yt_pred              #.....the prediction-ùë¶ÃÇ‚ü®ùë°‚ü©\n",
    "      list_accumulators.append(accumulator)#.....and the accumulator\n",
    "\n",
    "    acc = (list_accumulators,x)    #Store  and the list of accumulator into a tuple acc\n",
    "    return a, y_pred,acc \n",
    "\n",
    " ### WRITE SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Prv38PPNN7MS"
   },
   "source": [
    "## Task 3. Random RNN (1 point)\n",
    "Now that you have defined `rnn_cell_forward` and `rnn_forward`, let's test it with our data and inspect the results. This task consists of three exercises:\n",
    "* **(1 pt.)** First, initialize the RNN cell with the `series` provided above with $x$ of shape $(3, 10, 4)$. Remember also to define random values for the hidden layer $a$ and the weights and biases in the dictionary of parameters. Verify the output by printing `y_pred[1]` and the shape of `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RjAqe_dIN7MX"
   },
   "outputs": [],
   "source": [
    "def trend(time, slope=0):\n",
    "    return slope * time\n",
    "\n",
    "\n",
    "def seasonal_pattern(season_time):\n",
    "    \n",
    "    return np.where(season_time < 0.4,\n",
    "                    np.cos(season_time * 2 * np.pi),\n",
    "                    1 / np.exp(3 * season_time))\n",
    "\n",
    "\n",
    "def seasonality(time, period, amplitude=1, phase=0):\n",
    "    season_time = ((time + phase) % period) / period\n",
    "    return amplitude * seasonal_pattern(season_time)\n",
    "\n",
    "\n",
    "def noise(time, noise_level=1, seed=None):\n",
    "    rnd = np.random.RandomState(seed)\n",
    "    return rnd.randn(len(time)) * noise_level\n",
    "\n",
    "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
    "    plt.plot(time[start:end], series[start:end], format)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kEebbYdcN7Mp"
   },
   "outputs": [],
   "source": [
    "# Config of creating time series\n",
    "time = np.arange(120, dtype=\"float32\")\n",
    "baseline = 10\n",
    "series = trend(time, 0.1)\n",
    "baseline = 10\n",
    "amplitude = 40\n",
    "slope = 0.05\n",
    "noise_level = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0g5PJxwQN7M1"
   },
   "source": [
    "We initialize the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9a6aP3DN7M4"
   },
   "outputs": [],
   "source": [
    "# Create the time series\n",
    "series = baseline + trend(time, slope) + seasonality(time, period=365,amplitude=amplitude)\n",
    "# Update with noise\n",
    "series += noise(time, noise_level, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wYDaFu0KN7NJ"
   },
   "source": [
    "and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1050,
     "status": "ok",
     "timestamp": 1585192436912,
     "user": {
      "displayName": "Oluwafemi Olaleke",
      "photoUrl": "",
      "userId": "14193430993642365940"
     },
     "user_tz": -180
    },
    "id": "Af5vSFVkN7NP",
    "outputId": "43831e35-74f4-4bc0-f0d1-c224e77906d1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEHCAYAAABFroqmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_series(time,series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zg5cvMF-18KC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkbzlEEVRIEx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "prvxKiBeN7NZ"
   },
   "outputs": [],
   "source": [
    "### WRITE SOLUTION HERE ###\n",
    "                         #let na = 5, ny=2      \n",
    "\n",
    "x = torch.Tensor(series.reshape(3, 10, 4))  #with  ùë•  of shape (nx,m,Tx) ---> (3,10,4)\n",
    "a0 = torch.randn(5,10)   #random values for the hidden \"layer ùëé\"\n",
    "\n",
    "                        #initialize weight \n",
    "Wax = torch.randn(5,3)  # Wax--->(na,nx)\n",
    "Waa = torch.randn(5,5)  # Waa--->(na,na)\n",
    "Wya = torch.randn(2,5)  # Wya--->(ny,na) \n",
    "\n",
    "                        #initialize biases\n",
    "ba = torch.randn(5,1)   #ba--->(na,1)\n",
    "by = torch.randn(2,1)   #by--->(ny,1)\n",
    "dict_parameters = {\n",
    "              \"Wax\": Wax,\n",
    "              \"Waa\": Waa,\n",
    "              \"Wya\": Wya,\n",
    "              \"ba\": ba, \n",
    "              \"by\": by}\n",
    "\n",
    "### WRITE SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eoeX9JufN7Ns"
   },
   "outputs": [],
   "source": [
    "a,y_pred,accumulators  = rnn_forward(x, a0, dict_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 194
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1087,
     "status": "ok",
     "timestamp": 1585192546994,
     "user": {
      "displayName": "Oluwafemi Olaleke",
      "photoUrl": "",
      "userId": "14193430993642365940"
     },
     "user_tz": -180
    },
    "id": "sUqjyUCJoq0D",
    "outputId": "d7d708d9-8e87-43a4-c5a8-ed86834a036e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9790, 0.9790, 0.9790, 0.7871],\n",
       "        [0.7364, 0.9749, 0.7891, 0.9790],\n",
       "        [0.9790, 0.4829, 0.9790, 0.9774],\n",
       "        [0.4821, 0.9790, 0.9790, 0.9420],\n",
       "        [0.5483, 0.4898, 0.9790, 0.9789],\n",
       "        [0.4802, 0.4820, 0.4802, 0.4802],\n",
       "        [0.9497, 0.9156, 0.8362, 0.4812],\n",
       "        [0.9198, 0.4802, 0.4802, 0.4802],\n",
       "        [0.4802, 0.9790, 0.4802, 0.5043],\n",
       "        [0.4802, 0.4802, 0.4805, 0.4802]])"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[1]    #print y_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1061,
     "status": "ok",
     "timestamp": 1585192566896,
     "user": {
      "displayName": "Oluwafemi Olaleke",
      "photoUrl": "",
      "userId": "14193430993642365940"
     },
     "user_tz": -180
    },
    "id": "TF2s-wEeh-lI",
    "outputId": "ef7dac65-0cbe-4c71-bab5-93d6b5ce6443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DGhbPDJK2ABW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-UCDcGo1_1Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXABFkr1N7OH"
   },
   "source": [
    "# Home Assignment No. 4: Part 2 (Theory)\n",
    "In this part of the homework you are to solve several simple theoretical problems related to time series forecasting algorithms.\n",
    "* For every separate problem you can get only 0 points or maximal points for this problem. There are **NO INTERMEDIATE scores**.\n",
    "* Your solution must me **COMPLETE**, i.e. contain all required formulas/proofs/detailed explanations.\n",
    "* You must write your solution for any problem right after the words **YOUR SOLUTION**. Attaching pictures of your handwriting is allowed, but **highly discouraged**.\n",
    "## $\\LaTeX$ in Jupyter\n",
    "Jupyter has constantly improving $\\LaTeX$ support. Below are the basic methods to\n",
    "write **neat, tidy, and well typeset** equations in your notebooks:\n",
    "* to write an **inline** equation use \n",
    "```markdown\n",
    "$ you latex equation here $\n",
    "```\n",
    "* to write an equation, that is **displayed on a separate line** use \n",
    "```markdown\n",
    "$$ you latex equation here $$\n",
    "```\n",
    "* to write a **block of equations** use \n",
    "```markdown\n",
    "\\begin{align}\n",
    "    left-hand-side\n",
    "        &= right-hand-side on line 1\n",
    "        \\\\\n",
    "        &= right-hand-side on line 2\n",
    "        \\\\\n",
    "        &= right-hand-side on the last line\n",
    "\\end{align}\n",
    "```\n",
    "The **ampersand** (`&`) aligns the equations horizontally and the **double backslash**\n",
    "(`\\\\`) creates a new line.\n",
    "\n",
    "## The ARMA Process\n",
    "The family of ARMA processses (such as ARIMA and SARIMA) plays a major role in times series forecasting. In the first half of the homework, we will look deeper into understanding important elements around ARMA.\n",
    "\n",
    "We start by remembering that for a time series $\\{X_t\\}$ with $E(X^2_t) < \\infty$, its mean function is defined by $\\mu_X(t) = E(X_t)$ and the sample mean, for the $x_1, \\ldots, x_n$ observations by $\\bar{x} = \\frac{1}{n} \\sum^n_{t=1} x_t$. \n",
    "\n",
    "Similarly, its covariance function is \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma_X(r,s) = Cov(X_r, X_s) \\\\ \n",
    "= E[(X_r - \\mu_X(r))(X_s - \\mu_X(s))]\n",
    "\\end{aligned}\n",
    "$$ \n",
    "for all integers $r$ and $s$. \n",
    "\n",
    "We know that for $\\{X_t\\}$ to be (weakly) stationary, it must fulfill two conditions. First, $\\mu_X(t)$ must be independent of $t$, and $\\gamma_X(t+h, t)$ must be also independent of $t$ for each h.\n",
    "\n",
    "In the seminar, we applied the autocovariance and the autocorrelation functions to a dataset without deeper discussing their definition. Now, we look deeper into them.\n",
    "\n",
    "#### Autocovariance function\n",
    "The autocovariance function or ACVF of $\\{X_t\\}$ at lag $h$ is $\\gamma X(h) = Cov(X_{t+h}, X_t)$.\n",
    "\n",
    "For the observations $x_1, \\ldots, x_n$, the sample autocovariance function can be similarly defined by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\gamma} (h) := n^{-1} \\sum^{n - |h|}_{t=1} (x_{t+|h|} - \\bar{x})(x_t - \\bar{x})\n",
    "\\end{aligned}\n",
    "$$ \n",
    "for $-n < h < n$\n",
    "\n",
    "#### Autocorrelation function\n",
    "The autocorrelation function or ACF makes use of ACVF as following $\\rho_X(h) = \\frac{\\gamma X(h)}{\\gamma X(0)} = Cor(X_{t+h}, X_t)$.\n",
    "\n",
    "Similarly, we can write the sample autocorrelation function as \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\rho} (h) = \\frac{\\hat{\\gamma}(h)}{\\hat{\\gamma}(0)}\n",
    "\\end{aligned}\n",
    "$$ \n",
    "for $-n < h < n$\n",
    "\n",
    "Let's also remember a useful property of covariances, the linear property. It means that if that if $EX^2 < \\infty ,EY^2 < \\infty,EZ^2 < \\infty$ and $a$,$b$,and $c$ are any real constants, then, we can do $Cov(aX + bY + c, Z) = a Cov(X, Z) + b Cov(Y, Z)$.\n",
    "\n",
    "One of the cornerstones of classic forecasting is the Moving Average Process $MA(q)$. It can be defined, if the time series $\\{X_t\\}$ is $Xt = Z_t + \\theta_{1}Z_{t‚àí1} +\\ldots+ \\theta_{q}Z_{t‚àíq}$, where $\\{Z_t\\} \\sim WN(0,\\sigma^2)$, and $\\theta_{1},\\ldots,\\theta_{q}$ are constants, and $WN$, White Noise, being a sequence of uncorrelated random variables, each with zero mean and variance $\\sigma^2$ defined by $\\sigma^2 = E(X ‚àí \\mu)^2$. \n",
    "\n",
    "Let's put this into practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sW4v3IxcN7OP"
   },
   "source": [
    "## Task 1. MA(2) (1+1+1=3 points)\n",
    "Let's define a MA(2) with a white noise function of mean 0 and variance 1. Please do the following (**explicitly** outline all your computations):\n",
    "* **(1 pt.)** For $\\theta = 0.9$, derive the formula for the autocovariance and autocorrelation functions for this process.\n",
    "* **(1 pt.)** For $\\theta = 0.9$, compute the variance of the sample mean $(X_1 + X_2 + X_3 + X_4)/4$.\n",
    "* **(1 pt.)** If now we change our $\\theta$ to $-0.9$, what happens? What is the effect of a negative covariance? Please compare the result with the previous computation and **provide an explanation**.\n",
    "### Your Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36EGU4lwN7Rl"
   },
   "source": [
    "*** Please type here your solution ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJQYic9KH6XD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XX5CpAqNye1g"
   },
   "source": [
    "1.1] For  ùúÉ=0.9 , derive the formula for the autocovariance and autocorrelation functions for this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ka9MwS1H8Lz"
   },
   "source": [
    "    \n",
    "    \n",
    "    - The process can be expressed in the form shown below;\n",
    "\n",
    "$$\n",
    "X_t = Z_t + \\theta Z_{t-2}\n",
    "$$\n",
    "\n",
    "    - As stated in the problem statement; the autocovariance function can be expressed as;\n",
    "\n",
    "$$\n",
    "\\gamma_X(h) = Cov(X_{t+h}, X_t)\n",
    "$$\n",
    "\n",
    "    - And this expression is ;\n",
    "$$\n",
    "Cov(X_{t+h}, X_t) = E[X_{t+h} X_t] \n",
    "$$\n",
    "\n",
    "    - with appropriate substitution in the RHS of the eqn above;we get\n",
    "$$\n",
    "E[X_{t+h} X_t] = E[(Z_{t+h} + \\theta Z_{t+h-2})(Z_t + \\theta Z_{t-2})] \n",
    "$$\n",
    "    \n",
    "    - Further expansion gives; \n",
    "$$\n",
    "\\gamma_X(h) = E[Z_{t+h} Z_t] + \\theta E[Z_{t+h} Z_{t-2}] + \\theta E[Z_{t+h-2} Z_t] + \\theta^2 E[Z_{t+h-2} Z_{t-2}]\n",
    "$$\n",
    "\n",
    "    - Moving forward; some properties of Z_t worth noting;\n",
    "    \n",
    "    - Z_{t}'s variables(random) are uncorrelated, therefore;\n",
    "    \n",
    "$$Mean\\left(E[X_{t}]\\right) = 0$$  \n",
    "\n",
    "$$Cov(Z_{t_1}, Z_{t_2}) = 1\\;\\text{for}\\; t_1 = t_2 \\;\\text{and}\\;Cov(Z_{t_1}, Z_{t_2})= 0, \\;\\text{otherwise}$$ \n",
    " \n",
    "    - The AutoCovariance funtion; are dependent on h. \n",
    "    - The values of ACVF; for corresponding values of h is given as;\n",
    "\n",
    "$$\n",
    "\\gamma_X(h) =  1 + \\theta^2, \\; \\text{for}  h  = 0 \\\\\n",
    "\\gamma_X(h) =  0             \\; \\text{for} |h| = 1 \\\\\n",
    "\\gamma_X(h) =  \\theta        \\; \\text{for} |h| = 2 \\\\\n",
    "\\text{and}  \\; 0             \\;  \\text{otherwise} \n",
    "$$\n",
    "\n",
    "\n",
    "    - Dividing through by ACVF value at h=0,\n",
    "    - The autocorrelation function can be properly defined as:\n",
    "\n",
    "$$\n",
    "\\rho(h) = \\frac{\\gamma_X(h)}{\\gamma_X(0)} = \\left\\{\\begin{array}{ll} 1, \\text{for} h = 0 \\\\ 0, \\text{for} |h| = 1 \\\\ \\frac{\\theta}{1 + \\theta^2}, \\text{for} |h| = 2 \\\\ 0, \\text{otherwise} \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "\n",
    "     - Now with theta = 0.9, we obtain the corresponding solutions;\n",
    "\n",
    "$$\n",
    "\\gamma_X(h)  =  1.81\\;\\text{for}\\;  h  = 0 \\\\\n",
    "\\gamma_X(h)  =  0   \\;\\;\\;\\text{for}\\; |h| = 1 \\\\\n",
    "\\gamma_X(h)  =  0.9 \\;\\text{for}\\; |h| = 2 \\\\\n",
    "\\text{and}   \\; 0   \\;\\text{otherwise} \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\rho_X(h) = \\frac{\\gamma_X(h)}{\\gamma_X(0)} = \\left\\{\\begin{array}{ll} 1, \\text{if } h = 0 \\\\ 0, \\text{if } |h| = 1 \\\\ 0.487, \\text{if } |h| = 2 \\\\ 0, \\text{otherwise} \\end{array}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DH1GrZaayrdJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ifiXev5RyrAa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfn-QsOxysEL"
   },
   "source": [
    "1.2- For $\\theta = 0.9$, compute the variance of the sample mean $(X_1 + X_2 + X_3 + X_4)/4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofFCy3spyvql"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32S2ICjCywQ_"
   },
   "source": [
    "    \n",
    "    - As given in the task; \n",
    "    \n",
    "$$\n",
    "\\frac{(X_1 + X_2 + X_3 + X_4)}{4}\n",
    "$$\n",
    "\n",
    "    - The sample mean can be written as;\n",
    "\n",
    "$$\n",
    "\\frac{(X_1 + X_2 + X_3 + X_4)}{4} = \\frac{(Z_1 + \\theta Z_{-1} + Z_2 + \\theta Z_0 + Z_3 + \\theta Z_1 + Z_4 + \\theta Z_2)}{4} \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{(X_1 + X_2 + X_3 + X_4)}{4}\n",
    "=\\frac{(\\theta Z_{-1} + \\theta Z_0 + (1+\\theta) Z_1 + (1+\\theta) Z_2 + Z_3 + Z_4)}{4}\n",
    "$$\n",
    "\n",
    "    - The variance of the sample is expressed as;\n",
    "\n",
    "\n",
    "$$\n",
    "E\\left[ \\left(\\frac{X_1 + X_2 + X_3 + X_4}{4}\\right)^2 \\right] = \\frac{(\\theta Z_{-1} + \\theta Z_0 + (1+\\theta) Z_1 + (1+\\theta)Z_2 + Z_3 + Z_4)^2}{16} \n",
    "$$\n",
    "\n",
    "     - Further Expression \n",
    "\n",
    "$$\n",
    "E\\left[ \\left(\\frac{X_1 + X_2 + X_3 + X_4}{4}\\right)^2 \\right] = \\frac{(\\theta^2 + \\theta^2 + (1+\\theta)^2 + (1+\\theta)^2 + 1 + 1)}{16} = \\frac{(\\theta^2 + \\theta + 1)}{4}\n",
    "$$\n",
    "\n",
    "    - Knowing that theta= = 0.9\n",
    "\n",
    "$$\n",
    "E\\left[ \\left(\\frac{X_1 + X_2 + X_3 + X_4}{4}\\right)^2 \\right] = \\frac{(\\theta^2 + \\theta + 1)}{4} = 0.6775\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "niE_7aZEy3hG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUmYRlfGy3cP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j9v8hiKgy4SA"
   },
   "source": [
    "- (3) If now we change our  ùúÉ  to  ‚àí0.9 , \n",
    "\n",
    "- what happens? What is the effect of a negative covariance? \n",
    "\n",
    "- Please compare the result with the previous computation and provide an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFg604Eny6QO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrkJvDfQy605"
   },
   "source": [
    "\n",
    "    - with theta=-0.9, following the same procedure as above; the ACVF is given as;\n",
    "    \n",
    "$$\n",
    "\\gamma_X(h)  =  1.81\\;\\;\\text{for}\\;  h  = 0 \\\\\n",
    "\\gamma_X(h)  =  0 \\;    \\text{for}\\; |h| = 1 \\\\\n",
    "\\gamma_X(h)  =  -0.9 \\;\\text{for}\\; |h| = 2 \\\\\n",
    "\\text{and}   \\; 0\\;\\;\\;\\text{otherwise} \n",
    "$$\n",
    "    \n",
    "    - The AutoCorrelation Function \n",
    "\n",
    "$$\n",
    "\\rho_X(h) = \\frac{\\gamma_X(h)}{\\gamma_X(0)} = \\left\\{\\begin{array}{ll} 1, \\text{for} h = 0 \\\\ 0, \\text{for} |h| = 1 \\\\ -0.497, \\text{for} |h| = 2 \\\\ 0, \\text{otherwise} \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "    - The variance if therefore given as;\n",
    "$$\n",
    "E\\left[ \\left(\\frac{X_1 + X_2 + X_3 + X_4}{4}\\right)^2 \\right] = \\frac{(\\theta Z_{-1} + \\theta Z_0 + (1+\\theta) Z_1 + (1+\\theta) Z_2 + Z_3 + Z_4)^2 }{16} \n",
    "$$\n",
    "\n",
    "    - and this becomes;\n",
    "$$\n",
    "E\\left[ \\left(\\frac{X_1 + X_2 + X_3 + X_4}{4}\\right)^2 \\right] = \\frac{(\\theta^2 + \\theta + 1)}{4} = 0.2275\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9yjQnfBkzBF4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_bd8zqMTzCGF"
   },
   "source": [
    "    - what happens? What is the effect of a negative covariance?\n",
    "\n",
    "\n",
    "- ANSWER: The negative covariance has a decrease effect on the influence of $Z_1$ and $Z_2$.\n",
    "- Also, $Z_3$ and $Z_4$ become inversely propotional to $Z_1$ and $Z_2$ in some manners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v4N6K_G0zFBN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCES1Kl2zFnw"
   },
   "source": [
    "    - Please compare the result with the previous computation and provide an explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5cnDCvIzLEC"
   },
   "source": [
    "- The Moving Average process can be imagined as stream of batches of size 3\n",
    "- It is also not out of place to imagine it as moving over the WN -process $Z$\n",
    "- the MA has coefficients $1$, $0$, and $\\theta$ \n",
    "\n",
    "- The $\\gamma_X(0)$  is always +ve (and it remains +ve even after change of sign),$\\gamma_X(0)$  is equivalent to the covariance of $X_t$.  \n",
    "\n",
    "- The $\\gamma_X(1)$  is $0$ in senerio (i.e independent of  $\\theta$).it cam interpreted as the\"COV\" of 2-overlapping batches.(with 1-batch ahead of the other by 1-positional value) \n",
    "\n",
    "- $\\gamma_X(2)$ has a similar interpretation to  $\\gamma_X(1)$. its considered a covariance between  2-overlapping batches, but now with a difference pf 2-positional values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7nfVWlHmzOAX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqQevY4KzN69"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WXz-R4TN7Rq"
   },
   "source": [
    "## Task 2. Stationary processes (1+1=2 points)\n",
    "Assume that $\\{Y_t\\}$ represents a sequence with zero mean and variance $\\sigma^2$, let $a$ and $b$ are two constants. Are the following processes stationary? If they are, what would be their respective autocovariance and mean? \n",
    "* **(1 pt.)** $b + aY_0$\n",
    "* **(1 pt.)** $Y_{t}Y_{t-1}$\n",
    "\n",
    "Explicitly clarify your answer and please provide all necessary steps behind your reasoning.\n",
    "### Your Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IsqbNo9ON7R0"
   },
   "source": [
    "*** Please type here your solution ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yv5OAKrdzVC6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rVsOWNf1zUeX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQoTdjZKzWzl"
   },
   "source": [
    "- 2.1: For $b + a Y_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0wk8IWWtzbte"
   },
   "source": [
    "    -Two important conditions are to be verified before concluding if a process is weakly stationary or not;\n",
    " \n",
    "- $\\mu_X(t)$ is independent of $t$,$ \n",
    "\n",
    "- $\\gamma_X(t+h,t)$ must be independent of $t$ for every $h$ \n",
    "\n",
    " \n",
    "    - Checking the first condition;\n",
    "\n",
    "$$\n",
    "E\\left[X_t\\right] = E\\left[ b + a Y_0 \\right] = E\\left[ b\\right]  a E[Y_0]\n",
    "$$\n",
    "\n",
    "$$\n",
    "E\\left[X_t\\right] = b + a E[Y_0] =  b + a(0) = b\n",
    "$$\n",
    "\n",
    "    - Checking the second condition;\n",
    "\n",
    "$$\n",
    "\\gamma_X(r,s) = Cov\\left(X_{r}, X_{s}\\right) = Cov\\left((b+aY_{0}), (b+aY_{0})\\right)\n",
    "$$\n",
    "\n",
    "- with $r,s \\in Z_{+}$\n",
    "\n",
    "        - To compute the variance \n",
    "\n",
    "$$\n",
    "Var\\left(X\\right) = Cov\\left(X, X\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Var\\left(b + aY_{0}\\right) = a^{2} \\sigma^{2} \n",
    "$$\n",
    "\n",
    "- With both conditions satisfied, the process is stationary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfL0RnC2zezn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dXOXevxmzeNB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJs2hubYzfc6"
   },
   "source": [
    "-2.2] For  $Y_t Y_{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HeziYmxmzkjw"
   },
   "source": [
    "\n",
    "    - in verifying the first condition;\n",
    "    \n",
    "    - keeping in mind that with {Y_t} == sequence with zero-mean,   \n",
    "    \n",
    "    - then {Y_t}, {Y_t} == zero- mean independent variables \n",
    "\n",
    "$$\n",
    "\\mu_X = E[X_t] = E[Y_t Y_{t-1}] =E[Y_t]\\times E[Y_{t-1}] = 0\n",
    "$$\n",
    "\n",
    "     - for the verification of the 2nd condition;\n",
    "\n",
    "$$\n",
    "\\gamma_X(r,s) = Cov\\left(X_{r}, X_{s}\\right)\n",
    "$$\n",
    "\n",
    "    - and this is equivalent to; \n",
    "\n",
    "$$\n",
    "\\gamma_X(r,s) = E\\left[ \\left( Y_{r}Y_{r-1} -E[Y_{r}Y_{r-1}] \\right) \\left( Y_{s}Y_{s-1} -E[Y_{s}Y_{s-1}] \\right)\\right]   \n",
    "$$\n",
    "\n",
    "    - and this gives us;\n",
    "    \n",
    "$$\n",
    "\\gamma_X(r,s) = E\\left[\\left( Y_{r}Y_{r-1}Y_{s}Y_{s-1}\\right)\\right]   \n",
    "$$\n",
    "\n",
    "    - when r = s, \n",
    "    \n",
    "$$\n",
    "\\gamma_X(r,s) = \\sigma^{4}\n",
    "$$\n",
    "    \n",
    "    - otherwise we will have Expectation = 0, because we will have independent random variables  \n",
    "    \n",
    "- Now with $\\gamma_X(t+h,t)$ which is independent of $t$,\n",
    "- the process is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJ7GfaTO2RFG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "So_d9lkEN7R6"
   },
   "source": [
    "## Task 3. Questionable processes (1 point)\n",
    "Assume that we have two moving average processes MA(1) and $0 < |\\theta| < 1$. The first process is defined by $Q_t + \\frac{1}{\\theta}Q_{t-1}$. It has a White Noise with variance $\\sigma^2\\theta^2$ and zero mean. The second one is $S_t + \\theta S_{t‚àí1}$ with a White Noise with zero mean and variance $\\sigma^2$.\n",
    "\n",
    "Do they have the same autocovariance functions? Answer this question by deriving exact \n",
    "analytic formulas for autocovariation functions.\n",
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3H3ix4DrN7SA"
   },
   "source": [
    "*** Please type here your solution ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNpscmx7zrG8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZQ9cpHmzrB5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V420bw29zsQk"
   },
   "source": [
    "    - By analytically deriving the formulas for autovariation functions;\n",
    "    \n",
    "    \n",
    " - with $X_t = Q_t + \\frac{1}{\\theta} Q_{t-1}$ .\n",
    " \n",
    "        -The mean of X_t is given by:\n",
    "\n",
    "$$\n",
    "E[X_t] = E\\left[Q_t + \\frac{1}{\\theta} Q_{t-1}\\right] \n",
    "$$\n",
    "\n",
    "        - and this becomes...\n",
    "$$\n",
    "E[X_t] = E[Q_t] + \\frac{1}{\\theta}E[Q_{t-1}] = 0\n",
    "$$\n",
    "\n",
    "        - X_t AutoCovariance function is given as;\n",
    "\n",
    "\n",
    "$$\n",
    "\\gamma_X(t+h, t) = E\\left[ \\left( Q_{t+h} + \\frac{1}{\\theta} Q_{t+h-1} \\right) \\left( Q_{t} + \\frac{1}{\\theta} Q_{t-1} \\right) \\right] \n",
    "$$\n",
    "\n",
    "        - Expansion of the RHS of the equation gives...\n",
    "\n",
    "$$\n",
    "\\gamma_X(t+h, t) = \n",
    "E\\left[ Q_{t+h} Q_{t} + \\frac{1}{\\theta} Q_{t+h} Q_{t-1} + \\frac{1}{\\theta} Q_{t+h-1} Q_{t} + \\frac{1}{\\theta^2} Q_{t+h-1} Q_{t-1} \\right] \n",
    "$$\n",
    "\n",
    "        - AutoCovariance is given as; \n",
    "$$\n",
    "\\gamma_X(t+h, t)  = \\sigma^2 \\theta^2 + \\sigma^2  \\;\\;\\text{for}\\;h  = 0 \\\\\n",
    "\\gamma_X(t+h, t)) = \\sigma^2 \\theta, \\text{for} \\;\\; |h| = 1     \n",
    "$$\n",
    " \n",
    " \n",
    "         - with $Y_t = S_t + \\theta S_{t-1}$\n",
    "\n",
    "$$\n",
    "\\gamma_X(t+h, t) = = \\left\\{\\begin{array}{ll} \\sigma^2 \\theta^2 + \\sigma^2, \\text{if } h = 0 \\\\ \\sigma^2 \\theta, \\text{if } |h| = 1 \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "         - The mean of Y_t is given by:\n",
    "\n",
    "$$\n",
    "E[Y_t] = E[S_t + \\theta S_{t-1}]\n",
    "$$\n",
    "\n",
    "        - and this becomes...\n",
    "$$\n",
    "E[Y_t] = E[S_t] + \\theta E[S_{t-1}]  = 0\n",
    "$$\n",
    "\n",
    "        - Y_t AutoCovariance function is given as;\n",
    "\n",
    "\n",
    "$$\n",
    "\\gamma_Y(t+h, t) = E\\left[ \\left( S_{t+h} + \\theta S_{t+h-1} \\right) \\left( S_{t} + \\theta S_{t-1} \\right) \\right]\n",
    "$$\n",
    "\n",
    "        - Expansion of the RHS of the equation gives...\n",
    "\n",
    "$$\n",
    "\\gamma_X(t+h, t) =  = E\\left[ S_{t+h} S_{t} + \\theta S_{t+h} S_{t-1} + \\theta S_{t+h-1} S_{t} + \\theta^2 S_{t+h-1} S_{t-1} \\right] \n",
    "$$\n",
    "\n",
    "        - AutoCovariance is given as; \n",
    "$$\n",
    "\\gamma_Y(t+h, t)  = \\sigma^2 \\theta^2 + \\sigma^2 \\;\\;\\text{for}\\;h  = 0 \\\\\n",
    "\\gamma_Y(t+h, t)) = \\sigma^2 \\theta, \\text{for}\\; \\;\\; |h| = 1     \n",
    "$$\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "- It is seen from these derivations that the AutoCovariences are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYPlGZooz5Oq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oi5bM6SJz5JE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9GeKCp3N7SG"
   },
   "source": [
    "## ARMA\n",
    "Let's start by defining the ARMA(p, q) process for a time series ${Xt}$. This is the case if the time series is stationary and for every $t$, it satisfies\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X_t ‚àí \\phi X_{t‚àí1} - \\ldots -  \\phi_p X_{t‚àíp} = Z_{t} + \\theta Z_{t‚àí1} + \\ldots + \\theta_q Z_{t-q},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where ${Z_t} \\sim WN(0, \\sigma^2)$ In addition, the polynomials $1+ \\theta_{1}z + \\ldots + \\theta_{q}z^q$ and $1 ‚àí \\phi_{1}z ‚àí \\ldots ‚àí \\phi_{p}z^p$ have no common factors.\n",
    "\n",
    "\n",
    "\n",
    "If we define $B$ by the Backwards Shift Operator $BX_{t} = X_{t‚àí1}$ or more generally $B^j X_t = X_{t‚àíj}, B^j Z_t = Z_{t‚àíj}, j = 0, \\pm 1, \\ldots$, we can write this more cleanly as \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\phi(B)X_t = \\theta(B)Z_t,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\phi()$ and $\\theta()$ are the p-th and q-th-degree polynomials: $\\phi(z) = 1 ‚àí \\phi_1 z ‚àí \\ldots ‚àí \\phi_{p} z^p$ and\n",
    "$\\theta(z) = 1 + \\theta_1 z + \\ldots + \\theta_{q}z^q$.\n",
    "### The Partial Autocorrelation Function\n",
    "The partial autocorrelation function (PACF) of an ARMA process $\\{X_t\\}$ is the function $\\alpha()$ defined by the equations\n",
    "\n",
    "$$\\alpha(0) = 1 \\qquad \\text{and}\\qquad \\alpha(h) = \\phi_{hh}\\text{, } h \\geq 1,$$\n",
    "\n",
    "where $\\phi_{hh}$ is the last component of $\\phi_{h} = \\Gamma^{-1}_h \\gamma_h$. Here, $\\Gamma_h = [\\gamma(i - j)]^h_{i,j = 1}$ and $\\gamma_h = [\\gamma(1), \\gamma(2), \\ldots, \\gamma(h)]$\n",
    "\n",
    "Similarly, the sample PACF is the funciton $\\hat{\\alpha}()$, where \n",
    "\n",
    "$$\\hat{\\alpha}(0) = 1\\qquad \\text{and} \\qquad \\hat{\\alpha}(h) = \\hat{\\phi}_{hh}\\text{, } h \\geq 1,$$\n",
    "where $\\hat{\\phi}_{hh}$ is the last component of $\\hat{\\phi}_{h} = \\Gamma^{-1}_h \\hat{\\gamma}_h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmvnWpVszxai"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lp59ozSxzwY3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSBjLPiFN7SQ"
   },
   "source": [
    "## Task 4. ARMA (3 points)\n",
    "Assuming that the best linear predictor of $X_{h+1}$ can be estimated as $\\hat{X}_{h+1} = \\sum^h_{i=1} \\phi_{hi} X_{h+1-i}$, compute the PACF $\\alpha(2)$ of the moving average process MA(1) defined by $X_t = Q_t + \\theta Q_{t-1}$, where $\\{Q_t\\}$ is a White Noise with zero mean and variance $\\sigma^2$.\n",
    "### Your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FIrz2NKvN7SV"
   },
   "source": [
    "*** Please type here your solution ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DgjAq1vgN7Sc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBoLbeoC0BnR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dq62vTl00GI5"
   },
   "source": [
    "- Fist step:  evaluating $\\Gamma_2^{-1}$ \n",
    "   \n",
    "\n",
    "\n",
    "$$\\Gamma_2 = \\begin{bmatrix} \\gamma(0) & \\gamma(-1) \\\\ \\gamma(1) & \\gamma(0) \\end{bmatrix}$$\n",
    "\n",
    "- we then evaluate $\\Gamma_2^{-1} \\gamma_2$; \n",
    "\n",
    "$$\n",
    "\\Gamma_2^{-1} \\gamma_2 = \\begin{bmatrix} \\gamma(0) & \\gamma(-1) \\\\ \\gamma(1) & \\gamma(0) \\end{bmatrix}^{-1} \\begin{bmatrix} \\gamma(1) \\\\ \\gamma(2) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "- given that $\\Gamma_2^{-1} \\gamma_2$ = $\\alpha(2)$; \n",
    "\n",
    "\n",
    "$$\n",
    "\\alpha(2) = \\begin{bmatrix} \\gamma(0) & \\gamma(1) \\\\ \\gamma(1) & \\gamma(0) \\end{bmatrix}^{-1} \\begin{bmatrix} \\gamma(1) \\\\ \\gamma(2) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Gettin the expression for $\\gamma(0)$;\n",
    "\n",
    "$$\n",
    "\\gamma(0) = Cov(X_t, X_t) = E[(Q_t + \\theta Q_{t-1})^2] \n",
    "$$\n",
    "\n",
    "- Further expression gives... \n",
    "$$\n",
    "\\gamma(0) = E[Q_t^2 + 2 \\theta Q_t Q_{t-1} + \\theta^2 Q_{t-1}^2] \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gamma(0) = \\sigma^2 + \\theta^2 \\sigma^2\n",
    "$$\n",
    "\n",
    "\n",
    "- Gettin the expression for $\\gamma(1)$;\n",
    "\n",
    "$$\n",
    "\\gamma(1) = Cov(X_{t+1}, X_t) = E[(Q_{t+1} + \\theta Q_t)(Q_t + \\theta Q_{t-1})]$$\n",
    "\n",
    "$$\n",
    "\\gamma(1) = E[Q_{t+1} Q_t + \\theta Q_t^2 + \\theta Q_{t+1} Q_{t-1} + \\theta^2 Q_t Q_{t-1}] \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gamma(1) \\theta \\sigma^2\n",
    "$$ \n",
    "\n",
    "\n",
    "- Gettin the expression for $\\gamma(2)$;\n",
    "\n",
    "$$\n",
    "\\gamma(2) = Cov(X_{t+2}, X_t) = E[(Q_{t+2} + \\theta Q_{t+1})(Q_t + \\theta Q_{t-1})] \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gamma(2)= 0\n",
    "$$\n",
    "\n",
    "- Funding expresion for $\\Gamma_h^{-1}$:, recall that;\n",
    "\n",
    "\n",
    "$$\n",
    "\\Gamma_h^{-1} = \\begin{bmatrix} \\gamma(0) & \\gamma(1) \\\\ \\gamma(1) & \\gamma(0) \\end{bmatrix}^{-1}\n",
    "$$\n",
    "\n",
    "- Utilizing the schur properties obtain; \n",
    "$$\n",
    "\\Gamma_h^{-1} = \\left(\\gamma(0)^2 - \\gamma(1)^2  \\right)^{-1}\\begin{bmatrix} \\gamma(0) & -\\gamma(1) \\\\ -\\gamma(1) & \\gamma(0) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Now by substituting the appropriate values of  $\\gamma(0)$; $\\gamma(1)$; $\\gamma(2)$; derived above,\n",
    "\n",
    "$$\n",
    "\\Gamma_h^{-1}= \\left(   (\\sigma^2 + \\theta^2 \\sigma^2)^2 - (\\theta \\sigma^2)^2\\right)^{-1} \\begin{bmatrix} \\sigma^2 + \\theta^2 \\sigma^2 & -\\theta \\sigma^2 \\\\ -\\theta \\sigma^2 & \\sigma^2 + \\theta^2 \\sigma^2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- and now  the experession for $\\Gamma_2^{-1}\\gamma_2$ is gotten as;\n",
    "\n",
    "$$\n",
    "\\Gamma_2^{-1} \\gamma_2 = \\left( (\\sigma^2 + \\theta^2 \\sigma^2)^2 - (\\theta \\sigma^2)^2)\\right)^{-1} \\begin{bmatrix} \\sigma^2 + \\theta^2 \\sigma^2 & -\\theta \\sigma^2 \\\\ -\\theta \\sigma^2 & \\sigma^2 + \\theta^2 \\sigma^2 \\end{bmatrix} \\begin{bmatrix} \\theta \\sigma^2 \\\\ 0 \\end{bmatrix} \n",
    "$$\n",
    "\n",
    "- simplification by Matrix multiplication gives;\n",
    "\n",
    "$$\n",
    "\\Gamma_2^{-1} \\gamma_2 = \\left((\\sigma^2 + \\theta^2 \\sigma^2)^2 - (\\theta \\sigma^2)^2) \\right)^{-1}\\begin{bmatrix} (\\sigma^2 + \\theta^2 \\sigma^2) \\theta \\sigma^2 \\\\ -\\theta^2 \\sigma^4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- the PACF $\\alpha(2)$ is computed as; \n",
    "$$\n",
    "\\alpha(2) = \\frac{-\\theta^2 \\sigma^4}{(\\sigma^2 + \\theta^2 \\sigma^2)^2 - (\\theta \\sigma^2)^2} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha(2) = \\frac{-\\theta^2}{(1+ \\theta^2)^2 - (\\theta)^2} \n",
    "$$\n",
    "\n",
    "- Final expression is given as;\n",
    "\n",
    "$$\n",
    "\\alpha(2) = - \\left[ \\frac{\\theta^2}{1 + \\theta^2 + \\theta^4} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JceRzoX_0JAN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw4_Oluwafemi.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
